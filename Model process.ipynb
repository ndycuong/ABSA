{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8364714,"sourceType":"datasetVersion","datasetId":4971904},{"sourceId":8396685,"sourceType":"datasetVersion","datasetId":4995290},{"sourceId":8614776,"sourceType":"datasetVersion","datasetId":5156024},{"sourceId":8663782,"sourceType":"datasetVersion","datasetId":5191270}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T03:51:22.276258Z","iopub.execute_input":"2024-07-16T03:51:22.276656Z","iopub.status.idle":"2024-07-16T03:51:22.705249Z","shell.execute_reply.started":"2024-07-16T03:51:22.276597Z","shell.execute_reply":"2024-07-16T03:51:22.704251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch pandas","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:51:22.707270Z","iopub.execute_input":"2024-07-16T03:51:22.707901Z","iopub.status.idle":"2024-07-16T03:51:37.275148Z","shell.execute_reply.started":"2024-07-16T03:51:22.707861Z","shell.execute_reply":"2024-07-16T03:51:37.274101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thư viện vncorenlp và wordsegmenter dùng để Word Segmentation cho tiếng Việt\n\n# Install the vncorenlp python wrapper\n!pip install vncorenlp\n\n# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n!mkdir -p vncorenlp/models/wordsegmenter\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n!mv vi-vocab vncorenlp/models/wordsegmenter/\n!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:51:37.277417Z","iopub.execute_input":"2024-07-16T03:51:37.277764Z","iopub.status.idle":"2024-07-16T03:52:02.727336Z","shell.execute_reply.started":"2024-07-16T03:51:37.277732Z","shell.execute_reply":"2024-07-16T03:52:02.725900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tạo bộ word segmentation cho tiếng Việt\nfrom vncorenlp import VnCoreNLP\n\nrdrsegmenter = VnCoreNLP(\"/kaggle/working/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:02.729357Z","iopub.execute_input":"2024-07-16T03:52:02.730343Z","iopub.status.idle":"2024-07-16T03:52:08.180258Z","shell.execute_reply.started":"2024-07-16T03:52:02.730299Z","shell.execute_reply":"2024-07-16T03:52:08.179336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replace_list = {\n    'ô kêi': 'ok', 'okie': 'ok', 'o kê': 'ok', 'okey': 'ok', 'ôkê': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okê': 'ok',\n    'tks': 'cảm ơn', 'thks': 'cảm ơn', 'thanks': 'cảm ơn', 'ths': 'cảm ơn', 'thank': 'cảm ơn',\n    'kg': 'không', 'not': 'không', 'k': 'không', 'kh': 'không', 'kô': 'không', 'hok': 'không', 'ko': 'không', 'khong': 'không', 'kp': 'không phải',\n    'he he': 'tích cực', 'hehe': 'tích cực', 'hihi': 'tích cực', 'haha': 'tích cực', 'hjhj': 'tích cực', 'thick': 'tích cực',\n    'lol': 'tiêu cực', 'cc': 'tiêu cực', 'huhu': 'tiêu cực', 'cute': 'dễ thương',\n\n    'sz': 'cỡ', 'size': 'cỡ',\n    'wa': 'quá', 'wá': 'quá', 'qá': 'quá',\n    'đx': 'được', 'dk': 'được', 'dc': 'được', 'đk': 'được', 'đc': 'được',\n    'vs': 'với', 'j': 'gì', '“': ' ', 'time': 'thời gian', 'm': 'mình', 'mik': 'mình', 'r': 'rồi', 'bjo': 'bao giờ', 'very': 'rất',\n\n    'authentic': 'chuẩn chính hãng', 'aut': 'chuẩn chính hãng', 'auth': 'chuẩn chính hãng', 'date': 'hạn sử dụng', 'hsd': 'hạn sử dụng',\n    'store': 'cửa hàng', 'sop': 'cửa hàng', 'shopE': 'cửa hàng', 'shop': 'cửa hàng',\n    'sp': 'sản phẩm', 'product': 'sản phẩm', 'hàg': 'hàng',\n    'ship': 'giao hàng', 'delivery': 'giao hàng', 'síp': 'giao hàng', 'order': 'đặt hàng',\n\n    'gud': 'tốt', 'wel done': 'tốt', 'good': 'tốt', 'gút': 'tốt', 'tot': 'tốt', 'nice': 'tốt', 'perfect': 'rất tốt',\n    'quality': 'chất lượng', 'chất lg': 'chất lượng', 'chat': 'chất', 'excelent': 'hoàn hảo', 'bt': 'bình thường',\n    'sad': 'tệ', 'por': 'tệ', 'poor': 'tệ', 'bad': 'tệ',\n    'beautiful': 'đẹp tuyệt vời', 'dep': 'đẹp',\n    'xau': 'xấu', 'sấu': 'xấu',\n\n    'thik': 'thích', 'iu': 'yêu', 'fake': 'giả mạo',\n    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n    'fresh': 'tươi', 'delicious': 'ngon',\n\n    'dt': 'điện thoại', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khách sạn', 'nv': 'nhân viên',\n    'nt': 'nhắn tin', 'ib': 'nhắn tin', 'tl': 'trả lời', 'trl': 'trả lời', 'rep': 'trả lời',\n    'fback': 'feedback', 'fedback': 'feedback',\n    'sd': 'sử dụng', 'sài': 'xài',\n    'ctrai':'con trai',\n'khôg':'không',\n'bme':'bố mẹ',\n'cta':'chúng ta',\n'mih':'mình',\n'mqh':'mối quan hệ',\n'cgai':'con gái',\n'nhữg':'những',\n'mng':'mọi người',\n'svtn':'sinh viên tình nguyện',\n'r':'rồi',\n'qtam':'quan tâm',\n'thươg':'thương',\n'qtâm':'quan tâm',\n'chug':'chung',\n'trườg':'trường',\n'thoy':'thôi',\n'đki':'đăng ký',\n'atsm':'ảo tưởng sức mạnh',\n'ạk':'ạ',\n'cv':'công việc',\n'vch':'vãi chưởng',\n'cùg':'cùng',\n'pn':'bạn',\n'pjt':'biết',\n'thjk':'thích',\n'keke':'ce ce',\n'ktra':'kiểm tra',\n'nek':'nè',\n'cgái':'con gái',\n'nthe':'như thế',\n'chúg':'chúng',\n'kái':'cái',\n'tìh':'tình',\n'phòg':'phòng',\n'lòg':'lòng',\n'từg':'từng',\n'rằg':'rằng',\n'sốg':'sống',\n'thuj':'thôi',\n'thuơng':'thương',\n'càg':'càng',\n'đky':'đăng ký',\n'bằg':'bằng',\n'sviên':'sinh viên',\n'ák':'á',\n'đág':'đáng',\n'nvay':'như vậy',\n'nhjeu':'nhiều',\n'xg':'xuống',\n'zồi':'rồi',\n'trag':'trang',\n'zữ':'dữ',\n'atrai':'anh trai',\n'kte':'kinh tế',\n'độg':'động',\n'lmht':'liên minh huyền thoại',\n'gắg':'gắng',\n'đzai':'đẹp trai',\n'thgian':'thời gian',\n'plz':'pờ ly',\n'đồg':'đồng',\n'btrai':'bạn trai',\n'nthê':'như thế',\n'hìhì':'hì hì',\n'vọg':'vọng',\n'hihe':'hi he',\n'đôg':'đông',\n'răg':'răng',\n'thườg':'thường',\n'tcảm':'tình cảm',\n'đứg':'đứng',\n'ksao':'không sao',\n'dz':'đẹp trai',\n'hjxhjx':'hix hix',\n'cmày':'chúng mày',\n'xuốg':'xuống',\n'nkư':'như',\n'lquan':'liên quan',\n'tiếg':'tiếng',\n'hajz':'hai',\n'xih':'xinh',\n'hìh':'hình',\n'thàh':'thành',\n'ngke':'nghe',\n'dzậy':'dậy',\n'teencode':'tin cốt',\n'tnào':'thế nào',\n'tưởg':'tưởng',\n'ctrinh':'chương trình',\n'phog':'phong',\n'hôg':'không',\n'zìa':'gì',\n'kũg':'cũng',\n'ntnao':'như thế nào',\n'trọg':'trọng',\n'nthế':'như thế',\n'năg':'năng',\n'ngđó':'người đó',\n'lquen':'làm quen',\n'riêg':'riêng',\n'ngag':'ngang',\n'hêhê':'hê hê',\n'bnhiu':'bao nhiêu',\n'ngốk':'ngốc',\n'kậu':'cậu',\n'highland':'hai lừn',\n'kqua':'kết quả',\n'htrc':'hôm trước',\n'địh':'định',\n'gđình':'gia đinh',\n'giốg':'giống',\n'csống':'cuộc sống',\n'xug':'xùng',\n'zùi':'rồi',\n'bnhiêu':'bao nhiêu',\n'cbị':'chuẩn bị',\n'kòn':'còn',\n'buôg':'buông',\n'csong':'cuộc sống',\n'chàg':'chàng',\n'chăg':'chăng',\n'ngàh':'ngành',\n'llac':'liên lạc',\n'nkưng':'nhưng',\n'nắg':'nắng',\n'tíh':'tính',\n'khoảg':'khoảng',\n'thík':'thích',\n'ngđo':'người đó',\n'ngkhác':'người khác',\n'thẳg':'thẳng',\n'kảm':'cảm',\n'dàh':'dành',\n'júp':'giúp',\n'lặg':'lặng',\n'vđê':'vấn đề',\n'bbè':'bạn bè',\n'bóg':'bóng',\n'dky':'đăng ký',\n'dòg':'dòng',\n'uốg':'uống',\n'tyêu':'tình yêu',\n'snvv':'sinh nhật vui vẻ',\n'đthoại':'điện thoại',\n'qhe':'quan hệ',\n'cviec':'công việc',\n'tượg':'tượng',\n'qà':'quà',\n'thjc':'thích',\n'nhưq':'nhưng',\n'cđời':'cuộc đời',\n'bthường':'bình thường',\n'zà':'già',\n'đáh':'đánh',\n'xloi':'xin lỗi',\n'zám':'dám',\n'qtrọng':'quan trọng',\n'bìh':'bình',\n'lzi':'làm gì',\n'qhệ':'quan hệ',\n'đhbkhn':'đại học bách khoa hà nội',\n'hajzz':'hai',\n'kủa':'của',\n'lgi':'làm gì',\n'nvậy':'như vậy',\n'qả':'quả',\n'đkiện':'điều kiện',\n'nèk':'nè',\n'tlai':'tương lai',\n'bsĩ':'bác sĩ',\n'hkì':'học kỳ',\n'đcsvn':'đảng cộng sản việt nam',\n'vde':'vấn đề',\n'chta':'chúng ta',\n'òy':'rồi',\n\n    '^_^': 'tích cực', ':)': 'tích cực', ':(': 'tiêu cực',\n    '❤️': 'tích cực', '👍': 'tích cực', '🎉': 'tích cực', '😀': 'tích cực', '😍': 'tích cực', '😂': 'tích cực', '🤗': 'tích cực', '😙': 'tích cực', '🙂': 'tích cực',\n    '😔': 'tiêu cực', '😓': 'tiêu cực',\n    '⭐': 'star', '*': 'star', '🌟': 'star',\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.182964Z","iopub.execute_input":"2024-07-16T03:52:08.183592Z","iopub.status.idle":"2024-07-16T03:52:08.216216Z","shell.execute_reply.started":"2024-07-16T03:52:08.183565Z","shell.execute_reply":"2024-07-16T03:52:08.214969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n# import emoji\n# import string\n# emoji_pattern = re.compile(\"[\"\n#                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n#                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n#                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n#                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n#                 u\"\\U00002702-\\U000027B0\"\n#                 u\"\\U000024C2-\\U0001F251\"\n#                 u\"\\U0001f926-\\U0001f937\"\n#                 u'\\U00010000-\\U0010ffff'\n#                 u\"\\u200d\"\n#                 u\"\\u2640-\\u2642\"\n#                 u\"\\u2600-\\u2B55\"\n#                 u\"\\u23cf\"\n#                 u\"\\u23e9\"\n#                 u\"\\u231a\"\n#                 u\"\\u3030\"\n#                 u\"\\ufe0f\"\n#     \"]+\", flags=re.UNICODE)\n\n# def remove_unnecessary_characters(text):\n#     text = text.lower()\n#     text = re.sub(r\"[^a-zA-Zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ0-9\\s]\", \" \", text)\n# #     text = underthesea.word_tokenize(text, format=\"text\")\n#     text = re.sub(emoji_pattern, \" \", text)\n#     text = re.sub(r'([a-z]+?)\\1+',r'\\1', text)\n#     text = re.sub(r\"(\\w)\\s*([\" + string.punctuation + \"])\\s*(\\w)\", r\"\\1 \\2 \\3\", text)\n#     text = re.sub(r\"(\\w)\\s*([\" + string.punctuation + \"])\", r\"\\1 \\2\", text)\n#     text = re.sub(r\"(\\d)([^\\d.])\", r\"\\1 \\2\", text)\n#     text = re.sub(r\"([^\\d.])(\\d)\", r\"\\1 \\2\", text)\n#     text = re.sub(f\"([{string.punctuation}])([{string.punctuation}])+\",r\"\\1\", text)\n#     text = text.strip()\n#     while text.endswith(tuple(string.punctuation+string.whitespace)):\n#         text = text[:-1]\n#     while text.startswith(tuple(string.punctuation+string.whitespace)):\n#         text = text[1:]\n#     text = re.sub(r\"\\s+\", \" \", text)\n#     return text","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.217811Z","iopub.execute_input":"2024-07-16T03:52:08.218227Z","iopub.status.idle":"2024-07-16T03:52:08.230543Z","shell.execute_reply.started":"2024-07-16T03:52:08.218189Z","shell.execute_reply":"2024-07-16T03:52:08.229650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_acronyms(text):\n    words = []\n    for word in text.strip().split():\n        if word.lower() not in replace_list.keys(): words.append(word)\n        else: words.append(replace_list[word.lower()])\n    return emoji.demojize(' '.join(words))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.232231Z","iopub.execute_input":"2024-07-16T03:52:08.232620Z","iopub.status.idle":"2024-07-16T03:52:08.244119Z","shell.execute_reply.started":"2024-07-16T03:52:08.232568Z","shell.execute_reply":"2024-07-16T03:52:08.243075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport emoji\nimport string\n\n\ndef normalize_emoticons(text):\n    # Convert all forms of smile emoticons to a standard smile :)\n    text = re.sub(r\":\\)+\", \":)\", text)  \n    text = re.sub(r\":\\(+\", \":(\", text)\n\n    return emoji.demojize(text)\n\ndef remove_unnecessary_characters(text):\n    text = text.lower()\n    text = normalize_emoticons(text)\n    # Normalize extended repeated characters (e.g., đẹppp -> đẹp)\n    text = re.sub(r'([a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]{2,})\\1+', r'\\1', text)\n    text = re.sub(r'([0-9]+)\\1+', r'\\1', text)  # Normalize extended numbers\n    # Remove incorrect spaces around punctuation\n#     text = re.sub(r'\\s*([,.!?;:()]+)\\s*', r'\\1 ', text)\n    text = text.strip()\n    # Reduce multiple spaces to a single space\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[.,]', '', text)\n\n    text = normalize_acronyms(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.245333Z","iopub.execute_input":"2024-07-16T03:52:08.245581Z","iopub.status.idle":"2024-07-16T03:52:08.304253Z","shell.execute_reply.started":"2024-07-16T03:52:08.245558Z","shell.execute_reply":"2024-07-16T03:52:08.303099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Initialize the VnCoreNLP tokenizer\n\ndef segment_vietnamese(text):\n    segmented_text = rdrsegmenter.tokenize(text)\n    return ' '.join([' '.join(sentence) for sentence in segmented_text])\n\n\ndef preprocess_text(text):\n    text= text.lower()\n    text = segment_vietnamese(text)\n    text = remove_unnecessary_characters(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.305556Z","iopub.execute_input":"2024-07-16T03:52:08.305901Z","iopub.status.idle":"2024-07-16T03:52:08.312139Z","shell.execute_reply.started":"2024-07-16T03:52:08.305874Z","shell.execute_reply":"2024-07-16T03:52:08.311134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndfDev= pd.read_csv(r'/kaggle/input/absa-test/Dev.csv')\ndfTest= pd.read_csv(r'/kaggle/input/absa-test/Test.csv')\ndfTrain= pd.read_csv(r'/kaggle/input/absa-test/Train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:50:01.397560Z","iopub.execute_input":"2024-07-16T04:50:01.398253Z","iopub.status.idle":"2024-07-16T04:50:01.516233Z","shell.execute_reply.started":"2024-07-16T04:50:01.398222Z","shell.execute_reply":"2024-07-16T04:50:01.515352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfDev","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.505426Z","iopub.execute_input":"2024-07-16T03:52:08.505814Z","iopub.status.idle":"2024-07-16T03:52:08.535253Z","shell.execute_reply.started":"2024-07-16T03:52:08.505782Z","shell.execute_reply":"2024-07-16T03:52:08.534139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfDev= dfDev.copy()\ndfTest= dfTest.copy()\ndfTrain= dfTrain.copy()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:50:03.871791Z","iopub.execute_input":"2024-07-16T04:50:03.872613Z","iopub.status.idle":"2024-07-16T04:50:03.878235Z","shell.execute_reply.started":"2024-07-16T04:50:03.872571Z","shell.execute_reply":"2024-07-16T04:50:03.877264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### adding overall aspect","metadata":{}},{"cell_type":"code","source":"# Function to map star rating to sentiment\ndef map_sentiment(stars):\n    if stars in [1, 2]:\n        return 'Negative'\n    elif stars == 3:\n        return 'Neutral'\n    elif stars in [4, 5]:\n        return 'Positive'","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.545404Z","iopub.execute_input":"2024-07-16T03:52:08.545791Z","iopub.status.idle":"2024-07-16T03:52:08.554186Z","shell.execute_reply.started":"2024-07-16T03:52:08.545756Z","shell.execute_reply":"2024-07-16T03:52:08.553131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfDev['label'] = dfDev.apply(lambda row: f\"{row['label']}{{OVERALL#{map_sentiment(row['n_star'])}}};\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.555393Z","iopub.execute_input":"2024-07-16T03:52:08.555703Z","iopub.status.idle":"2024-07-16T03:52:08.587219Z","shell.execute_reply.started":"2024-07-16T03:52:08.555678Z","shell.execute_reply":"2024-07-16T03:52:08.586202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTest['label'] = dfTest.apply(lambda row: f\"{row['label']}{{OVERALL#{map_sentiment(row['n_star'])}}};\", axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.588519Z","iopub.execute_input":"2024-07-16T03:52:08.588968Z","iopub.status.idle":"2024-07-16T03:52:08.634108Z","shell.execute_reply.started":"2024-07-16T03:52:08.588935Z","shell.execute_reply":"2024-07-16T03:52:08.633126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTrain['label'] = dfTrain.apply(lambda row: f\"{row['label']}{{OVERALL#{map_sentiment(row['n_star'])}}};\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:08.635323Z","iopub.execute_input":"2024-07-16T03:52:08.635653Z","iopub.status.idle":"2024-07-16T03:52:08.774347Z","shell.execute_reply.started":"2024-07-16T03:52:08.635599Z","shell.execute_reply":"2024-07-16T03:52:08.773385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfSA= pd.read_csv(r'/kaggle/input/data38k/ecmdata.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:56:59.062067Z","iopub.execute_input":"2024-07-16T05:56:59.062808Z","iopub.status.idle":"2024-07-16T05:56:59.260014Z","shell.execute_reply.started":"2024-07-16T05:56:59.062777Z","shell.execute_reply":"2024-07-16T05:56:59.259049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:57:00.029758Z","iopub.execute_input":"2024-07-16T05:57:00.030377Z","iopub.status.idle":"2024-07-16T05:57:00.034738Z","shell.execute_reply.started":"2024-07-16T05:57:00.030347Z","shell.execute_reply":"2024-07-16T05:57:00.033663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndfSA['label'] = dfSA['label'].str.lower()\ndfSA['label'] = dfSA['label'].replace({'pos': 1, 'neu': 2, 'neg': 0})\ndfSA['label'] = label_encoder.fit_transform(dfSA['label'])","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:57:00.626106Z","iopub.execute_input":"2024-07-16T05:57:00.626829Z","iopub.status.idle":"2024-07-16T05:57:00.678317Z","shell.execute_reply.started":"2024-07-16T05:57:00.626794Z","shell.execute_reply":"2024-07-16T05:57:00.677190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfSA","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:41:55.339177Z","iopub.execute_input":"2024-07-16T01:41:55.339522Z","iopub.status.idle":"2024-07-16T01:41:55.351901Z","shell.execute_reply.started":"2024-07-16T01:41:55.339491Z","shell.execute_reply":"2024-07-16T01:41:55.350988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfSA['comment'] = dfSA['comment'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:57:04.204156Z","iopub.execute_input":"2024-07-16T05:57:04.204983Z","iopub.status.idle":"2024-07-16T05:58:46.934633Z","shell.execute_reply.started":"2024-07-16T05:57:04.204950Z","shell.execute_reply":"2024-07-16T05:58:46.933442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts =dfSA['label'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:31.735819Z","iopub.execute_input":"2024-07-16T01:43:31.736205Z","iopub.status.idle":"2024-07-16T01:43:31.748396Z","shell.execute_reply.started":"2024-07-16T01:43:31.736170Z","shell.execute_reply":"2024-07-16T01:43:31.747533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:46.937076Z","iopub.execute_input":"2024-07-16T05:58:46.937457Z","iopub.status.idle":"2024-07-16T05:58:46.942063Z","shell.execute_reply.started":"2024-07-16T05:58:46.937425Z","shell.execute_reply":"2024-07-16T05:58:46.941145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfSA_train, dfSA_temp = train_test_split(dfSA, test_size=0.2, random_state=42, stratify=dfSA['label'])\ndfSA_val, dfSA_test = train_test_split(dfSA_temp, test_size=0.5, random_state=42, stratify=dfSA_temp['label'])","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:46.943392Z","iopub.execute_input":"2024-07-16T05:58:46.943768Z","iopub.status.idle":"2024-07-16T05:58:46.981470Z","shell.execute_reply.started":"2024-07-16T05:58:46.943736Z","shell.execute_reply":"2024-07-16T05:58:46.980744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_counts = dfSA_train['label'].value_counts().sort_index()\nval_counts = dfSA_val['label'].value_counts().sort_index()\ntest_counts = dfSA_test['label'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:31.884324Z","iopub.execute_input":"2024-07-16T01:43:31.884607Z","iopub.status.idle":"2024-07-16T01:43:31.892136Z","shell.execute_reply.started":"2024-07-16T01:43:31.884584Z","shell.execute_reply":"2024-07-16T01:43:31.891149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts= dfSA['label'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:31.893314Z","iopub.execute_input":"2024-07-16T01:43:31.893634Z","iopub.status.idle":"2024-07-16T01:43:31.903720Z","shell.execute_reply.started":"2024-07-16T01:43:31.893600Z","shell.execute_reply":"2024-07-16T01:43:31.903040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_counts,val_counts,test_counts, counts","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:31.905111Z","iopub.execute_input":"2024-07-16T01:43:31.905473Z","iopub.status.idle":"2024-07-16T01:43:31.916051Z","shell.execute_reply.started":"2024-07-16T01:43:31.905443Z","shell.execute_reply":"2024-07-16T01:43:31.915141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nlabels = ['Negative', 'Positive', 'Neutral']\n\n# Plotting the pie charts side by side\nfig, axes = plt.subplots(1, 3, figsize=(24, 8))\n\n# Colors\ncolors = ['#ff9999','#66b3ff','#99ff99']\nexplode = (0.1, 0, 0)\n\n# Train set\naxes[0].pie(train_counts, explode=explode, colors=colors, autopct='%1.1f%%',\n            shadow=True, startangle=90)\naxes[0].axis('equal')\naxes[0].set_title('Train Set')\n\n# Validation set\naxes[1].pie(val_counts, explode=explode, colors=colors, autopct='%1.1f%%',\n            shadow=True, startangle=90)\naxes[1].axis('equal')\naxes[1].set_title('Validation Set')\n\n# Test set\naxes[2].pie(test_counts, explode=explode, colors=colors, autopct='%1.1f%%',\n            shadow=True, startangle=90)\naxes[2].axis('equal')\naxes[2].set_title('Test Set')\n\n# Adding a main title\nfig.suptitle('Tỷ lệ phần trăm cảm xúc trong dữ liệu')\n\n# Adding a legend to the figure\nfig.legend(labels, loc='center right', fontsize='large')\n\nplt.subplots_adjust(right=0.85)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:31.917266Z","iopub.execute_input":"2024-07-16T01:43:31.917592Z","iopub.status.idle":"2024-07-16T01:43:32.447680Z","shell.execute_reply.started":"2024-07-16T01:43:31.917559Z","shell.execute_reply":"2024-07-16T01:43:32.446826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\ndef plot_simple_pie_chart(sizes, labels, title):\n    colors = ['#ff9999','#66b3ff','#99ff99']\n    explode = (0.1, 0, 0)  # explode the 1st slice\n\n    plt.figure(figsize=(8, 8))\n    plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n            shadow=True, startangle=90)\n    plt.axis('equal')\n    plt.title(title)\n    plt.legend(labels, loc='center right', fontsize='large')\n    plt.subplots_adjust(right=0.85)\n    plt.show()\nlabels = ['Negative', 'Positive', 'Neutral']\n\n# Plotting the simple pie charts with the provided counts\nplot_simple_pie_chart(counts, labels, 'Label Distribution - Dataset')\n# plot_simple_pie_chart(val_counts, labels, 'Label Distribution - Validation Set')\n# plot_simple_pie_chart(test_counts, labels, 'Label Distribution - Test Set')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:32.454041Z","iopub.execute_input":"2024-07-16T01:43:32.454367Z","iopub.status.idle":"2024-07-16T01:43:32.683945Z","shell.execute_reply.started":"2024-07-16T01:43:32.454341Z","shell.execute_reply":"2024-07-16T01:43:32.682690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfSA_test['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:32.685748Z","iopub.execute_input":"2024-07-16T01:43:32.686516Z","iopub.status.idle":"2024-07-16T01:43:32.698246Z","shell.execute_reply.started":"2024-07-16T01:43:32.686460Z","shell.execute_reply":"2024-07-16T01:43:32.696916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndfSA['sentence_length'] = dfSA['comment'].apply(lambda x: len(x.split()))\n\nplt.figure(figsize=(10, 6))\nplt.hist(dfSA['sentence_length'], bins=100, color='pink')\nplt.xlabel('Sentence Length (words)')\nplt.ylabel('Frequency')\nplt.grid(axis='y')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:32.700469Z","iopub.execute_input":"2024-07-16T01:43:32.701412Z","iopub.status.idle":"2024-07-16T01:43:33.089273Z","shell.execute_reply.started":"2024-07-16T01:43:32.701387Z","shell.execute_reply":"2024-07-16T01:43:33.088345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calulate the average sentence length\naverage_sentence_length = dfSA['sentence_length'].mean()\naverage_sentence_length","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:33.090648Z","iopub.execute_input":"2024-07-16T01:43:33.091010Z","iopub.status.idle":"2024-07-16T01:43:33.098312Z","shell.execute_reply.started":"2024-07-16T01:43:33.090975Z","shell.execute_reply":"2024-07-16T01:43:33.097282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming dfSA is a pandas dataframe with a 'text' column containing sentences.\n# Calculate the sentence length (number of words in each sentence)\ndfSA['sentence_length'] = dfSA['comment'].apply(lambda x: len(x.split()))\n\n# Calculate the average sentence length\naverage_sentence_length = dfSA['sentence_length'].mean()\n\n# Calculate the 5th and 95th percentiles\npercentile_5 = dfSA['sentence_length'].quantile(0.05)\npercentile_95 = dfSA['sentence_length'].quantile(0.95)\n\nprint(\"Average sentence length:\", average_sentence_length)\nprint(\"5th percentile sentence length:\", percentile_5)\nprint(\"95th percentile sentence length:\", percentile_95)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:33.099498Z","iopub.execute_input":"2024-07-16T01:43:33.099773Z","iopub.status.idle":"2024-07-16T01:43:33.190759Z","shell.execute_reply.started":"2024-07-16T01:43:33.099749Z","shell.execute_reply":"2024-07-16T01:43:33.189911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:46.983907Z","iopub.execute_input":"2024-07-16T05:58:46.984218Z","iopub.status.idle":"2024-07-16T05:58:46.989445Z","shell.execute_reply.started":"2024-07-16T05:58:46.984192Z","shell.execute_reply":"2024-07-16T05:58:46.988357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:46.990639Z","iopub.execute_input":"2024-07-16T05:58:46.990956Z","iopub.status.idle":"2024-07-16T05:58:47.481986Z","shell.execute_reply.started":"2024-07-16T05:58:46.990932Z","shell.execute_reply":"2024-07-16T05:58:47.481126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CSADataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = tokenizer(text, max_length=75, padding='max_length', truncation=True, return_tensors=\"pt\")\n\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze() #remove all singleton dimensions 1\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n    \ntrain_sadataset = CSADataset(dfSA_train['comment'].tolist(), dfSA_train['label'].tolist())\ntrain_saloader = DataLoader(train_sadataset, batch_size=24, shuffle=True)\nval_sadataset = CSADataset(dfSA_val['comment'].tolist(), dfSA_val['label'].tolist())\nval_saloader = DataLoader(val_sadataset, batch_size=24, shuffle=False) \ntest_sadataset = CSADataset(dfSA_test['comment'].tolist(), dfSA_test['label'].tolist())\ntest_saloader = DataLoader(test_sadataset, batch_size=24, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:47.483277Z","iopub.execute_input":"2024-07-16T05:58:47.483577Z","iopub.status.idle":"2024-07-16T05:58:47.496902Z","shell.execute_reply.started":"2024-07-16T05:58:47.483552Z","shell.execute_reply":"2024-07-16T05:58:47.495969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_saloader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:47.498304Z","iopub.execute_input":"2024-07-16T05:58:47.498675Z","iopub.status.idle":"2024-07-16T05:58:47.527348Z","shell.execute_reply.started":"2024-07-16T05:58:47.498641Z","shell.execute_reply":"2024-07-16T05:58:47.526466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTest['label'][15]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:41.007086Z","iopub.execute_input":"2024-07-16T01:43:41.007463Z","iopub.status.idle":"2024-07-16T01:43:41.014372Z","shell.execute_reply.started":"2024-07-16T01:43:41.007430Z","shell.execute_reply":"2024-07-16T01:43:41.013377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:41.015667Z","iopub.execute_input":"2024-07-16T01:43:41.015976Z","iopub.status.idle":"2024-07-16T01:43:41.022613Z","shell.execute_reply.started":"2024-07-16T01:43:41.015942Z","shell.execute_reply":"2024-07-16T01:43:41.021602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/absa-test/Dev.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:43:41.023827Z","iopub.execute_input":"2024-07-16T01:43:41.024859Z","iopub.status.idle":"2024-07-16T01:43:41.044188Z","shell.execute_reply.started":"2024-07-16T01:43:41.024828Z","shell.execute_reply":"2024-07-16T01:43:41.043435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the mappings\n# aspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4, 'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9, 'OVERALL':10}\nsentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\naspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4, 'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\n\n# Preprocess the labels\ndef preprocess_labels(label_str):\n    labels = re.findall(r'\\{(.*?)\\}', label_str)\n    aspects = torch.zeros(len(aspect_to_index), dtype=torch.long)\n    sentiments = torch.full((len(aspect_to_index),), sentiment_to_index['None'], dtype=torch.long)\n\n    for label in labels:\n        try:\n            if '#' in label:\n                aspect, sentiment = label.split('#')\n            else:\n                aspect = label\n                sentiment = 'None'  # Default for 'OTHERS'\n\n            if aspect == 'OTHERS':\n                continue  # Skip the \"OTHERS\" aspect\n\n            idx = aspect_to_index[aspect]\n            aspects[idx] = 1\n            sentiments[idx] = sentiment_to_index[sentiment]\n        except KeyError as e:\n            print(f\"Error processing label: {label}. Error: {str(e)}\")\n            continue\n\n    return aspects, sentiments","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:50:32.105971Z","iopub.execute_input":"2024-07-16T04:50:32.106665Z","iopub.status.idle":"2024-07-16T04:50:32.116062Z","shell.execute_reply.started":"2024-07-16T04:50:32.106631Z","shell.execute_reply":"2024-07-16T04:50:32.115097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_labels('{FEATURES#Negative};{GENERAL#Positive};{OTHERS};')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:52:57.756514Z","iopub.execute_input":"2024-07-16T03:52:57.757265Z","iopub.status.idle":"2024-07-16T03:52:57.810553Z","shell.execute_reply.started":"2024-07-16T03:52:57.757229Z","shell.execute_reply":"2024-07-16T03:52:57.809503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply preprocessing\ndfTest['comment'] = dfTest['comment'].apply(preprocess_text)\ndfTest['aspects'], dfTest['sentiments'] = zip(*dfTest['label'].apply(preprocess_labels))\n# Apply preprocessing\ndfTrain['comment'] = dfTrain['comment'].apply(preprocess_text)\ndfTrain['aspects'], dfTrain['sentiments'] = zip(*dfTrain['label'].apply(preprocess_labels))\n# Apply preprocessing\ndfDev['comment'] = dfDev['comment'].apply(preprocess_text)\ndfDev['aspects'], dfDev['sentiments'] = zip(*dfDev['label'].apply(preprocess_labels))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:50:36.249829Z","iopub.execute_input":"2024-07-16T04:50:36.250524Z","iopub.status.idle":"2024-07-16T04:51:17.309679Z","shell.execute_reply.started":"2024-07-16T04:50:36.250490Z","shell.execute_reply":"2024-07-16T04:51:17.308582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTest['sentiments'][250]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:53:46.934672Z","iopub.execute_input":"2024-07-16T03:53:46.934981Z","iopub.status.idle":"2024-07-16T03:53:46.942933Z","shell.execute_reply.started":"2024-07-16T03:53:46.934955Z","shell.execute_reply":"2024-07-16T03:53:46.941942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTest","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:18.680851Z","iopub.execute_input":"2024-07-16T01:44:18.681285Z","iopub.status.idle":"2024-07-16T01:44:18.734734Z","shell.execute_reply.started":"2024-07-16T01:44:18.681253Z","shell.execute_reply":"2024-07-16T01:44:18.733774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aggregate_sentiment_counts(df):\n    sentiment_counts = torch.zeros((len(aspect_to_index), len(sentiment_to_index)), dtype=torch.long)\n    for sentiments in df['sentiments']:\n        for aspect_idx, sentiment in enumerate(sentiments):\n            sentiment_counts[aspect_idx, sentiment] += 1\n    return sentiment_counts\n\ntrain_sentiment_counts = aggregate_sentiment_counts(dfTrain)\nval_sentiment_counts = aggregate_sentiment_counts(dfDev)\ntest_sentiment_counts = aggregate_sentiment_counts(dfTest)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:18.735877Z","iopub.execute_input":"2024-07-16T01:44:18.736196Z","iopub.status.idle":"2024-07-16T01:44:20.351077Z","shell.execute_reply.started":"2024-07-16T01:44:18.736171Z","shell.execute_reply":"2024-07-16T01:44:20.350312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_sentiment_distribution(train_counts, val_counts, test_counts):\n    aspects = list(aspect_to_index.keys())\n    sentiments = ['Negative', 'Positive', 'Neutral']\n\n    fig, axes = plt.subplots(2, 5, figsize=(20, 10), sharey=True)\n    axes = axes.flatten()\n\n    for i, aspect in enumerate(aspects):\n        ax = axes[i]\n        counts_train = train_counts[i].tolist()[:3]\n        counts_val = val_counts[i].tolist()[:3]\n        counts_test = test_counts[i].tolist()[:3]\n\n        bar_width = 0.25\n        bar1 = range(len(counts_train))\n        bar2 = [x + bar_width for x in bar1]\n        bar3 = [x + bar_width for x in bar2]\n\n        ax.bar(bar1, counts_train, color='#ff9999', width=bar_width, edgecolor='grey', label='Train')\n        ax.bar(bar2, counts_val, color='#66b3ff', width=bar_width, edgecolor='grey', label='Dev')\n        ax.bar(bar3, counts_test, color='#99ff99', width=bar_width, edgecolor='grey', label='Test')\n\n        # Adding text annotations\n        for j in range(len(counts_train)):\n            ax.text(bar1[j], counts_train[j] + 1, str(counts_train[j]), ha='center', color='black')\n            ax.text(bar2[j], counts_val[j] + 1, str(counts_val[j]), ha='center', color='black')\n            ax.text(bar3[j], counts_test[j] + 1, str(counts_test[j]), ha='center', color='black')\n\n        ax.set_title(aspect)\n        ax.set_xticks([r + bar_width for r in range(len(counts_train))])\n        ax.set_xticklabels(sentiments)\n\n    handles, labels = ax.get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper right', fontsize='large')\n#     plt.suptitle('Sentiment Distribution Across Aspects in Train, Val, and Test Sets')\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:20.353921Z","iopub.execute_input":"2024-07-16T01:44:20.354207Z","iopub.status.idle":"2024-07-16T01:44:20.367479Z","shell.execute_reply.started":"2024-07-16T01:44:20.354183Z","shell.execute_reply":"2024-07-16T01:44:20.366470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_sentiment_distribution(train_sentiment_counts, val_sentiment_counts, test_sentiment_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:20.368654Z","iopub.execute_input":"2024-07-16T01:44:20.368921Z","iopub.status.idle":"2024-07-16T01:44:22.467837Z","shell.execute_reply.started":"2024-07-16T01:44:20.368899Z","shell.execute_reply":"2024-07-16T01:44:22.466914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculate sentence lengths for each dataset\ndfTrain['sentence_length'] = dfTrain['comment'].apply(lambda x: len(x.split()))\ndfDev['sentence_length'] = dfDev['comment'].apply(lambda x: len(x.split()))\ndfTest['sentence_length'] = dfTest['comment'].apply(lambda x: len(x.split()))\n\n# Plot the combined distribution of sentence lengths for each dataset\nplt.figure(figsize=(12, 6))\n\nplt.hist(dfTrain['sentence_length'], bins=50, color='#ff9999', edgecolor='grey', alpha=0.5, label='Train')\nplt.hist(dfDev['sentence_length'], bins=50, color='red', edgecolor='grey', alpha=0.5, label='Validation')\nplt.hist(dfTest['sentence_length'], bins=50, color='blue', edgecolor='grey', alpha=0.5, label='Test')\n\nplt.xlabel('Sentence Length (words)')\nplt.ylabel('Frequency')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:22.469151Z","iopub.execute_input":"2024-07-16T01:44:22.469457Z","iopub.status.idle":"2024-07-16T01:44:23.224737Z","shell.execute_reply.started":"2024-07-16T01:44:22.469430Z","shell.execute_reply":"2024-07-16T01:44:23.223858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:54:02.553034Z","iopub.execute_input":"2024-07-16T03:54:02.554075Z","iopub.status.idle":"2024-07-16T03:54:02.558637Z","shell.execute_reply.started":"2024-07-16T03:54:02.554038Z","shell.execute_reply":"2024-07-16T03:54:02.557480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommentDataset(Dataset):\n    def __init__(self, comments, labels):\n        self.comments = comments\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.comments)\n\n    def __getitem__(self, idx):\n        comment = self.comments[idx]\n        aspects, sentiments = preprocess_labels(self.labels[idx])\n        inputs = tokenizer(comment, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n        return {\n            'input_ids': inputs['input_ids'].squeeze(0), # removes only the first dimension if it is a singleton\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'aspects': aspects,\n            'sentiments': sentiments\n        }\n    \ntrain_dataset = CommentDataset(dfTrain['comment'].tolist(), dfTrain['label'].tolist())\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataset = CommentDataset(dfDev['comment'].tolist(), dfDev['label'].tolist())\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False) \ntest_dataset = CommentDataset(dfTest['comment'].tolist(), dfTest['label'].tolist())\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:51:17.311668Z","iopub.execute_input":"2024-07-16T04:51:17.311947Z","iopub.status.idle":"2024-07-16T04:51:17.323648Z","shell.execute_reply.started":"2024-07-16T04:51:17.311923Z","shell.execute_reply":"2024-07-16T04:51:17.322622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n\n# # Assuming encoding contains tensors with singleton dimensions\n# encoding = {\n#     'input_ids': torch.tensor([[[1, 2, 3]]]),  # Shape: (1, 1, 3)\n#     'attention_mask': torch.tensor([[[1, 1, 1]]])  # Shape: (1, 1, 3)\n# }\n# print(encoding['input_ids'])\n\n# print(\"Input IDs shape:\", encoding['input_ids'].shape) \n# print(\"Attention Mask shape:\", encoding['attention_mask'].shape)\n\n# input_ids = encoding['input_ids'].squeeze()\n# attention_mask = encoding['attention_mask'].squeeze()\n# print(input_ids)\n# print(\"Input IDs shape:\", input_ids.shape)  # Output: torch.Size([3])\n# print(\"Attention Mask shape:\", attention_mask.shape)  # Output: torch.Size([3])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:23.245462Z","iopub.execute_input":"2024-07-16T01:44:23.245767Z","iopub.status.idle":"2024-07-16T01:44:23.254939Z","shell.execute_reply.started":"2024-07-16T01:44:23.245721Z","shell.execute_reply":"2024-07-16T01:44:23.254138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n\n# # Assuming inputs contains tensors with a batch dimension of size 1\n# inputs = {\n#     'input_ids': torch.tensor([[1, 2, 3]]),  # Shape: (1, 3)\n#     'attention_mask': torch.tensor([[1, 1, 1]])  # Shape: (1, 3)\n# }\n\n# processed_inputs = {\n#     'input_ids': inputs['input_ids'].squeeze(0),\n#     'attention_mask': inputs['attention_mask'].squeeze(0),\n# }\n\n# print(\"Input IDs shape:\", processed_inputs['input_ids'].shape)  # Output: torch.Size([3])\n# print(\"Attention Mask shape:\", processed_inputs['attention_mask'].shape)  # Output: torch.Size([3])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:23.255997Z","iopub.execute_input":"2024-07-16T01:44:23.256295Z","iopub.status.idle":"2024-07-16T01:44:23.264114Z","shell.execute_reply.started":"2024-07-16T01:44:23.256272Z","shell.execute_reply":"2024-07-16T01:44:23.263149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n\n# Example sentences\nsentences = [\"This is a test.\", \"Another test sentence.\"]\n\n# Tokenize with batching\nencoding = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\nprint(encoding['input_ids']) #Each number corresponds to a token in the sentence according to the tokenizer’s vocabulary\nprint(encoding['attention_mask']) # 1s for positions with actual tokens and 0s for positions with padding tokens.\nprint(\"Input IDs shape:\", encoding['input_ids'].shape)\nprint(\"Attention Mask shape:\", encoding['attention_mask'].shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:23.265202Z","iopub.execute_input":"2024-07-16T01:44:23.265510Z","iopub.status.idle":"2024-07-16T01:44:23.911232Z","shell.execute_reply.started":"2024-07-16T01:44:23.265481Z","shell.execute_reply":"2024-07-16T01:44:23.910339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in test_saloader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:23.912468Z","iopub.execute_input":"2024-07-16T01:44:23.912753Z","iopub.status.idle":"2024-07-16T01:44:23.935643Z","shell.execute_reply.started":"2024-07-16T01:44:23.912717Z","shell.execute_reply":"2024-07-16T01:44:23.934780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in test_loader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:44:23.936938Z","iopub.execute_input":"2024-07-16T01:44:23.937242Z","iopub.status.idle":"2024-07-16T01:44:23.962513Z","shell.execute_reply.started":"2024-07-16T01:44:23.937217Z","shell.execute_reply":"2024-07-16T01:44:23.961601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:51:17.324758Z","iopub.execute_input":"2024-07-16T04:51:17.325012Z","iopub.status.idle":"2024-07-16T04:51:17.339362Z","shell.execute_reply.started":"2024-07-16T04:51:17.324991Z","shell.execute_reply":"2024-07-16T04:51:17.338503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# # Class counts\n# class_counts = [8737, 22554, 5521]\n\n# # Calculate class weights\n# class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n# class_weights = class_weights / class_weights.sum()  # Normalize the weights\n# class_weights = class_weights.to(device)  # Move to the same device as your model\n\n# # Define the criterion with the calculated weights\n# criterion_csa = nn.CrossEntropyLoss(weight=class_weights)\n\n# # Print the weights for verification\n# print(f\"Class weights: {class_weights}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.088628Z","iopub.status.idle":"2024-07-14T17:52:55.089002Z","shell.execute_reply.started":"2024-07-14T17:52:55.088818Z","shell.execute_reply":"2024-07-14T17:52:55.088834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define the training function as described previously\n# def train(model, train_loader,train_saloader, val_loader,val_saloader, optimizer, scheduler, device, num_epochs=20, patience=5):\n#     train_losses, val_losses = [], []\n#     model.to(device)\n#     criterion_aspect = nn.BCEWithLogitsLoss()\n#     criterion_sentiment = nn.CrossEntropyLoss()    \n#     criterion_csa = nn.CrossEntropyLoss(weight=class_weights)\n#     best_val_loss = float('inf')\n#     epochs_without_improvement = 0\n\n#     for epoch in range(num_epochs):\n#         model.train()\n#         total_train_loss = 0\n#         for batch in train_loader:\n#             input_ids = batch['input_ids'].to(device)\n#             attention_mask = batch['attention_mask'].to(device)\n#             aspect_labels = batch['aspects'].to(device).float()  # shape: (batch_size, n_aspects)\n#             sentiment_labels = batch['sentiments'].to(device).long()\n\n#             optimizer.zero_grad()\n#             logits,_ = model(input_ids, attention_mask)\n            \n#             # Aspect detection loss\n#             aspect_logits = logits[:, :, 0]  # Assuming the first column in logits is for aspect detection\n# #             print(aspect_logits,'and',aspect_labels)\n\n#             aspect_loss = criterion_aspect(aspect_logits, aspect_labels)\n# #             print(aspect_logits,'111:\\n', aspect_logits)\n            \n#             # Sentiment classification loss\n#             sentiment_logits = logits.view(-1,4)  # Flatten the logits for CrossEntropyLoss\n# #             print(sentiment_logits, 'and1',sentiment_labels.view(-1))\n\n#             sentiment_loss = criterion_sentiment(sentiment_logits, sentiment_labels.view(-1))\n# #             print(sentiment_logits,'111:\\n', sentiment_labels.view(-1))\n    \n#             # Total loss\n#             absa_loss = aspect_loss + sentiment_loss\n#             absa_loss.backward()\n#             optimizer.step()\n#             scheduler.step()\n#             total_train_loss += absa_loss.item()\n            \n#         for batch in train_saloader:\n#             input_ids = batch['input_ids'].to(device)\n#             attention_mask = batch['attention_mask'].to(device)\n#             labels = batch['labels'].to(device)\n#             optimizer.zero_grad()\n\n#             _, csa_logits = model(input_ids, attention_mask)\n# #             print(csa_logits,'and2',labels)\n         \n#             csa_loss = criterion_csa(csa_logits, labels)\n            \n#             csa_loss.backward()\n#             optimizer.step()\n#             scheduler.step()\n#             total_train_loss += csa_loss.item()\n\n#         avg_train_loss = total_train_loss / (len(train_loader) + len(train_saloader))\n#         train_losses.append(avg_train_loss)\n#         print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train absa Loss: {absa_loss:.4f}, Train csa Loss: {csa_loss:.4f}')\n\n#         model.eval()\n#         total_train_val_loss = 0\n#         with torch.no_grad():\n#             for batch in val_loader:\n#                 input_ids = batch['input_ids'].to(device)\n#                 attention_mask = batch['attention_mask'].to(device)\n#                 aspect_labels = batch['aspects'].to(device).float()  # shape: (batch_size, n_aspects)\n#                 sentiment_labels = batch['sentiments'].to(device).long()\n                \n#                 logits,_ = model(input_ids, attention_mask)\n\n#                 aspect_logits = logits[:, :, 0]  # Assuming the first column in logits is for aspect detection\n#                 aspect_loss = criterion_aspect(aspect_logits, aspect_labels)\n#     #             print(aspect_logits,'111:\\n', aspect_logits)\n\n#                 # Sentiment classification loss\n#                 sentiment_logits = logits.view(-1,4)  # Flatten the logits for CrossEntropyLoss\n\n#                 sentiment_loss = criterion_sentiment(sentiment_logits, sentiment_labels.view(-1))\n#     #             print(sentiment_logits,'111:\\n', sentiment_labels.view(-1))\n#                 # Total loss\n#                 absa_loss = aspect_loss + sentiment_loss\n#                 total_train_val_loss += absa_loss.item()\n                \n#             for batch in val_saloader:\n#                 input_ids = batch['input_ids'].to(device)\n#                 attention_mask = batch['attention_mask'].to(device)\n#                 labels = batch['labels'].to(device)\n\n#                 _, csa_logits = model(input_ids, attention_mask)\n\n#                 csa_loss = criterion_csa(csa_logits, labels)\n\n#                 total_train_val_loss += csa_loss.item()\n\n#         avg_val_loss = total_train_val_loss / (len(val_loader) + len(val_saloader))\n#         val_losses.append(avg_val_loss)\n#         print(f'Validation Loss: {avg_val_loss:.4f},Train absa Loss: {absa_loss:.4f}, Train cá Loss: {csa_loss:.4f}')\n\n#         if avg_val_loss < best_val_loss:\n#             best_val_loss = avg_val_loss\n#             torch.save(model.state_dict(), 'best_model.pth')\n#             epochs_without_improvement = 0\n#         else:\n#             epochs_without_improvement += 1\n#             if epochs_without_improvement >= patience:\n#                 print(f\"Early stopping after {epochs_without_improvement} epochs without improvement.\")\n#                 break\n\n#     return train_losses, val_losses","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.090832Z","iopub.status.idle":"2024-07-14T17:52:55.091237Z","shell.execute_reply.started":"2024-07-14T17:52:55.091018Z","shell.execute_reply":"2024-07-14T17:52:55.091033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch in val_saloader:\n#     print(batch)\n#     break\n# #     input_ids = batch['input_ids'].to(device)\n# #     attention_mask = batch['attention_mask'].to(device)\n# #     labels = batch['labels'].to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.092936Z","iopub.status.idle":"2024-07-14T17:52:55.093405Z","shell.execute_reply.started":"2024-07-14T17:52:55.093188Z","shell.execute_reply":"2024-07-14T17:52:55.093204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AdamW, get_linear_schedule_with_warmup\n# optimizer = AdamW(model.parameters(), lr=2e-5)\n# total_steps = len(train_loader) * 20  # Assuming 20 epochs\n# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.094912Z","iopub.status.idle":"2024-07-14T17:52:55.095306Z","shell.execute_reply.started":"2024-07-14T17:52:55.095118Z","shell.execute_reply":"2024-07-14T17:52:55.095137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_losses, val_losses = train(model, train_loader,train_saloader, val_loader,val_saloader, optimizer, scheduler, device, num_epochs=20, patience=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.096302Z","iopub.status.idle":"2024-07-14T17:52:55.096673Z","shell.execute_reply.started":"2024-07-14T17:52:55.096489Z","shell.execute_reply":"2024-07-14T17:52:55.096505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_predictions, aspect_true_labels, absasentiment_predictions, absasentiment_true_labels,csasentiment_predictions,csasentiment_true_labels= evaluate(model, test_loader, test_saloader, device, sentiment_to_index)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.107404Z","iopub.status.idle":"2024-07-14T17:52:55.107795Z","shell.execute_reply.started":"2024-07-14T17:52:55.107608Z","shell.execute_reply":"2024-07-14T17:52:55.107623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display_metrics_per_aspect_final(aspect_predictions, aspect_true_labels, absasentiment_predictions,absasentiment_true_labels,  aspect_to_index,sentiment_to_index)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.112272Z","iopub.status.idle":"2024-07-14T17:52:55.112631Z","shell.execute_reply.started":"2024-07-14T17:52:55.112456Z","shell.execute_reply":"2024-07-14T17:52:55.112470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report, accuracy_score\n# print(classification_report(csasentiment_true_labels, csasentiment_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.114650Z","iopub.status.idle":"2024-07-14T17:52:55.115037Z","shell.execute_reply.started":"2024-07-14T17:52:55.114851Z","shell.execute_reply":"2024-07-14T17:52:55.114867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dftest= pd.read_csv('/kaggle/input/data38k/ecmdata.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.118022Z","iopub.status.idle":"2024-07-14T17:52:55.118423Z","shell.execute_reply.started":"2024-07-14T17:52:55.118237Z","shell.execute_reply":"2024-07-14T17:52:55.118252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dftest['comment'][1]","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.119910Z","iopub.status.idle":"2024-07-14T17:52:55.120487Z","shell.execute_reply.started":"2024-07-14T17:52:55.120116Z","shell.execute_reply":"2024-07-14T17:52:55.120138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comment= 'Pin kém còn lại miễn chê mua 8/3/2019 tình trạng pin còn 88% có ai giống tôi không'\n\n# predicted_aspects, predicted_csa_sentiments = predict(model, tokenizer, device, comment)\n\n# for i, (aspects, csa_sentiment) in enumerate(zip(predicted_aspects, predicted_csa_sentiments)):\n#     print(f\"Predicted Aspects and Sentiments: {aspects}\")\n#     print(f\"Predicted CSA Sentiment: {csa_sentiment}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.121841Z","iopub.status.idle":"2024-07-14T17:52:55.122247Z","shell.execute_reply.started":"2024-07-14T17:52:55.122024Z","shell.execute_reply":"2024-07-14T17:52:55.122039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- # END combined absa+csa TEST -->","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.130863Z","iopub.status.idle":"2024-07-14T17:52:55.131245Z","shell.execute_reply.started":"2024-07-14T17:52:55.131034Z","shell.execute_reply":"2024-07-14T17:52:55.131049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.133380Z","iopub.status.idle":"2024-07-14T17:52:55.133779Z","shell.execute_reply.started":"2024-07-14T17:52:55.133585Z","shell.execute_reply":"2024-07-14T17:52:55.133600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AdamW, get_linear_schedule_with_warmup\n# optimizer1 = AdamW(model.parameters(), lr=2e-5)\n# total_steps = len(train_loader) * 40  # Assuming 20 epochs\n# scheduler1 = get_linear_schedule_with_warmup(optimizer1, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.136450Z","iopub.status.idle":"2024-07-14T17:52:55.136833Z","shell.execute_reply.started":"2024-07-14T17:52:55.136642Z","shell.execute_reply":"2024-07-14T17:52:55.136658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_losses, val_losses = train(model, train_loader, val_loader, optimizer1, scheduler1, device, num_epochs=40, patience=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:52:55.138340Z","iopub.status.idle":"2024-07-14T17:52:55.138746Z","shell.execute_reply.started":"2024-07-14T17:52:55.138541Z","shell.execute_reply":"2024-07-14T17:52:55.138569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### predict","metadata":{}},{"cell_type":"code","source":"# Example of target with class indices\nloss = nn.CrossEntropyLoss()\ninput = torch.randn(3, 5, requires_grad=True)\nprint(input)\nimport torch.nn.functional as F\nprint(F.softmax(input, dim=1))\ntarget = torch.empty(3, dtype=torch.long).random_(5)\nprint(target)\noutput = loss(input, target)\nprint(output)\noutput.backward()\n# # Example of target with class probabilities\n# input = torch.randn(3, 5, requires_grad=True)\n# print(input)\n# target = torch.randn(3, 5).softmax(dim=1)\n# print(target)\n# output = loss(input, target)\n# print(output)\n# output.backward()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T18:19:22.861654Z","iopub.execute_input":"2024-07-14T18:19:22.862051Z","iopub.status.idle":"2024-07-14T18:19:22.873309Z","shell.execute_reply.started":"2024-07-14T18:19:22.862019Z","shell.execute_reply":"2024-07-14T18:19:22.872042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ## endtest -->","metadata":{}},{"cell_type":"markdown","source":"# ABSA begin","metadata":{}},{"cell_type":"code","source":"# aspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4, 'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\naspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4, 'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9, 'OVERALL':10}\n\nsentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\nindex_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\n\nindex_to_aspect = {v: k for k, v in aspect_to_index.items()}\naspect_names = list(aspect_to_index.keys())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:57:42.435594Z","iopub.execute_input":"2024-07-16T03:57:42.436042Z","iopub.status.idle":"2024-07-16T03:57:42.443148Z","shell.execute_reply.started":"2024-07-16T03:57:42.436011Z","shell.execute_reply":"2024-07-16T03:57:42.442110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4 last hidden","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from transformers import AutoModel\n\n# class NewApproach(nn.Module):\n#     def __init__(self, num_aspects=10, num_polarities=4, hidden_dim=3072):\n#         super().__init__()\n#         self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\", output_hidden_states=True)\n#         self.mlp = nn.Sequential(\n#             nn.Linear(self.phobert.config.hidden_size * 4, hidden_dim),\n#             nn.ReLU(),\n#             nn.Dropout(0.2)\n#         )\n#         self.aspect_classifiers = nn.ModuleList([\n#             nn.Linear(hidden_dim, num_polarities) for _ in range(num_aspects)\n#         ])\n#         self.num_aspects = num_aspects\n#         self.num_polarities = num_polarities\n\n#     def forward(self, input_ids, attention_mask):\n#         outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n#         concat_hidden = torch.cat(outputs.hidden_states[-4:], dim=-1)  # Shape: (batch_size, seq_len, hidden_size*4)\n#         cls_output = concat_hidden[:, 0, :]  # Shape: (batch_size, hidden_size*4)\n#         cls_output = self.mlp(cls_output)\n        \n#         # Calculate logits for each aspect\n#         aspect_logits = [classifier(cls_output) for classifier in self.aspect_classifiers]\n        \n#         # Concatenate the aspect logits along the last dimension\n#         logits = torch.cat(aspect_logits, dim=1)\n#         logits = logits.view(-1, self.num_aspects, self.num_polarities)\n\n#         return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T03:58:40.266804Z","iopub.execute_input":"2024-07-14T03:58:40.267160Z","iopub.status.idle":"2024-07-14T03:58:40.273958Z","shell.execute_reply.started":"2024-07-14T03:58:40.267137Z","shell.execute_reply":"2024-07-14T03:58:40.273183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass MultibranchABSA(nn.Module):\n    def __init__(self, num_aspects=10, num_polarities=4):\n        super().__init__()\n        self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\", output_hidden_states=True)\n        self.dropout = nn.Dropout(0.2)\n        self.classifiers = nn.ModuleList([\n            nn.Linear(self.phobert.config.hidden_size * 4, num_polarities) for _ in range(num_aspects)\n        ])\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        concat_hidden = torch.cat(outputs.hidden_states[-4:], dim=-1)  # Shape: (batch_size, seq_len, hidden_size*4)\n        cls_output = concat_hidden[:, 0, :]  # Shape: (batch_size, hidden_size*4)\n        cls_output = self.dropout(cls_output)\n        logits = torch.stack([classifier(cls_output) for classifier in self.classifiers], dim=1)\n#         logits = [classifier(cls_output) for classifier in self.classifiers]\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-15T02:42:18.011895Z","iopub.execute_input":"2024-07-15T02:42:18.012212Z","iopub.status.idle":"2024-07-15T02:42:18.021516Z","shell.execute_reply.started":"2024-07-15T02:42:18.012182Z","shell.execute_reply":"2024-07-15T02:42:18.020733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass MultitaskABSA(nn.Module):\n    def __init__(self, num_aspects=10, num_polarities=4):\n        super().__init__()\n        self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\", output_hidden_states=True)\n        self.dropout = nn.Dropout(0.2)\n        # Instead of a classifier for each aspect, use a single classifier that handles all aspects\n        self.classifier = nn.Linear(self.phobert.config.hidden_size * 4, num_aspects * num_polarities)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        concat_hidden = torch.cat(outputs.hidden_states[-4:], dim=-1)\n        cls_output = concat_hidden[:, 0, :]  # Use only the [CLS] output\n        cls_output = self.dropout(cls_output)\n        logits = self.classifier(cls_output)\n        # Reshape logits to have dimensions (batch_size, num_aspects, num_polarities)\n        logits = logits.view(-1, 10, 4)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:46:19.833856Z","iopub.execute_input":"2024-07-16T01:46:19.834699Z","iopub.status.idle":"2024-07-16T01:46:19.845614Z","shell.execute_reply.started":"2024-07-16T01:46:19.834657Z","shell.execute_reply":"2024-07-16T01:46:19.844536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultitaskABSAConcat(nn.Module):\n    def __init__(self, num_aspects=11, num_polarities=4):\n        super().__init__()\n        self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\", output_hidden_states=True)\n        self.dropout = nn.Dropout(0.2)\n        self.aspect_classifiers = nn.ModuleList([\n            nn.Linear(self.phobert.config.hidden_size * 4, num_polarities) for _ in range(num_aspects)\n        ])\n        self.num_aspects= num_aspects\n        self.num_polarities= num_polarities\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask,  output_hidden_states=True)\n        concat_hidden = torch.cat(outputs.hidden_states[-4:], dim=-1)  # Shape: (batch_size, seq_len, hidden_size*4)\n        cls_output = concat_hidden[:, 0, :]  # Shape: (batch_size, hidden_size*4)\n        cls_output = self.dropout(cls_output)\n        \n        # Calculate logits for each aspect\n        aspect_logits = [classifier(cls_output) for classifier in self.aspect_classifiers]\n        \n        # Concatenate the aspect logits along the last dimension\n        logits = torch.cat(aspect_logits, dim=1)\n        logits = logits.view(-1, self.num_aspects, self.num_polarities)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:59:43.169342Z","iopub.execute_input":"2024-07-16T03:59:43.169793Z","iopub.status.idle":"2024-07-16T03:59:43.182959Z","shell.execute_reply.started":"2024-07-16T03:59:43.169760Z","shell.execute_reply":"2024-07-16T03:59:43.181765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sample input\nbatch_size = 16\nseq_len = 256\ninput_ids = torch.randint(0, 1000, (batch_size, seq_len))\nattention_mask = torch.ones((batch_size, seq_len))\n\nmodel_multibranch = MultibranchABSA(num_aspects=10, num_polarities=4)\nmodel_concat = MultitaskABSAConcat(num_aspects=10, num_polarities=4)\nmodel_multitask = MultitaskABSA(num_aspects=10, num_polarities=4)\n# Generate outputs\noutputs_multibranch = model_multibranch(input_ids, attention_mask)\noutputs_concat = model_concat(input_ids, attention_mask)\noutputs_multitask = model_multitask(input_ids, attention_mask)\n\n# Print outputs\nprint(\"MultibranchABSA output shape:\", outputs_multibranch.shape)\nprint(\"\\nMultitaskABSAConcat output shape:\", outputs_concat.shape)\nprint(\"\\nMultitaskABSA output shape:\", outputs_multitask.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T04:03:57.710820Z","iopub.execute_input":"2024-07-14T04:03:57.711210Z","iopub.status.idle":"2024-07-14T04:04:13.643691Z","shell.execute_reply.started":"2024-07-14T04:03:57.711179Z","shell.execute_reply":"2024-07-14T04:04:13.642590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"MultibranchABSA output shape:\", outputs_multibranch[0])\n# print(\"\\nMultitaskABSAConcat output shape:\", outputs_concat[0])\n# print(\"\\nMultitaskABSA output shape:\", outputs_multitask[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T04:29:39.767201Z","iopub.execute_input":"2024-07-14T04:29:39.767875Z","iopub.status.idle":"2024-07-14T04:29:39.771886Z","shell.execute_reply.started":"2024-07-14T04:29:39.767840Z","shell.execute_reply":"2024-07-14T04:29:39.770975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- ### just use origin: adam with weigh-decay=0.01 -->","metadata":{}},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T03:59:00.073758Z","iopub.execute_input":"2024-07-14T03:59:00.074260Z","iopub.status.idle":"2024-07-14T03:59:00.101381Z","shell.execute_reply.started":"2024-07-14T03:59:00.074229Z","shell.execute_reply":"2024-07-14T03:59:00.100483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_loader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-14T04:34:52.798681Z","iopub.execute_input":"2024-07-14T04:34:52.799057Z","iopub.status.idle":"2024-07-14T04:34:52.823696Z","shell.execute_reply.started":"2024-07-14T04:34:52.799027Z","shell.execute_reply":"2024-07-14T04:34:52.822781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the model, criterion, and optimizer\nmodel = MultitaskABSAConcat(num_aspects=10, num_polarities=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n\n# Set device to CUDA if available\nmodel.to(device)\n\n# Example of processing one batch\nfor batch in train_loader:\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    sentiments = batch['sentiments'].to(device)\n    print('se',sentiments)\n\n    optimizer.zero_grad()\n    outputs = model(input_ids, attention_mask)  # This should be a single tensor now\n    print('o',outputs)\n    loss = 0\n    for i in range(outputs.shape[1]):  # Iterate over each aspect\n        loss += criterion(outputs[:, i, :], sentiments[:, i])\n        print(outputs[:, i, :])\n        print(sentiments[:, i])\n        print(loss)\n\n    # Print the outputs and loss\n        print(\"Outputs shape:\", outputs.shape)\n        print(\"Loss:\", loss.item())\n        break  # Just process one batch for this e\n    break\n'''tensor([[-0.1526, -0.0674,  0.2411,  0.1065],\n        [-0.0548, -0.0465,  0.3515,  0.1444],\n        [-0.1727,  0.1266,  0.3029,  0.1133],\n        [ 0.0375,  0.0809,  0.3087,  0.2032],\n        [ 0.0060, -0.0743,  0.2716,  0.1565],\n        [-0.0580,  0.1228,  0.1853, -0.0122],\n        [-0.0251,  0.0960,  0.3207,  0.1585],\n        [ 0.0092,  0.0067,  0.1007,  0.1176],\n        [ 0.0576, -0.1359,  0.4195,  0.0727],\n        [-0.0487,  0.0264,  0.1734,  0.0826],\n        [-0.0885,  0.0263,  0.2352,  0.1001],\n        [ 0.0332, -0.0066,  0.2499,  0.2246],\n        [-0.0051,  0.0358,  0.1585,  0.0326],\n        [-0.0489, -0.0470,  0.3628,  0.0262],\n        [-0.0216,  0.1041,  0.2702, -0.0063],\n        [-0.0724, -0.0241,  0.1483,  0.1142]], device='cuda:0',\n       grad_fn=<SliceBackward0>)\ntensor([3, 1, 3, 2, 3, 2, 1, 0, 0, 0, 0, 3, 2, 2, 3, 0], device='cuda:0')'''","metadata":{"execution":{"iopub.status.busy":"2024-07-14T04:37:28.242898Z","iopub.execute_input":"2024-07-14T04:37:28.243283Z","iopub.status.idle":"2024-07-14T04:37:28.829023Z","shell.execute_reply.started":"2024-07-14T04:37:28.243255Z","shell.execute_reply":"2024-07-14T04:37:28.828157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# # Sample predictions and labels\n# predictions = torch.tensor([[-0.1508, -0.0144, -0.1146, -0.0415],\n#                             [-0.0674, -0.0657, -0.0633, -0.0135],\n#                             [-0.0055,  0.0223, -0.0400, -0.0950],\n#                             [ 0.0037,  0.0225, -0.0924, -0.2033],\n#                             [-0.0806,  0.0098, -0.2648, -0.1741],\n#                             [-0.1511, -0.0042, -0.1899, -0.1380],\n#                             [-0.1143, -0.0272, -0.0723, -0.0984],\n#                             [-0.1456, -0.0070, -0.1005, -0.0336],\n#                             [-0.2205,  0.0230, -0.0437, -0.0593],\n#                             [-0.0005,  0.0460, -0.2211, -0.0957],\n#                             [-0.1531,  0.1311, -0.0162, -0.0652],\n#                             [-0.0349,  0.0098, -0.0914, -0.0318],\n#                             [-0.2264,  0.0451,  0.1416, -0.1238],\n#                             [-0.1334,  0.0085, -0.0709, -0.0355],\n#                             [-0.0522, -0.0064, -0.1147, -0.0847],\n#                             [-0.1338, -0.0464,  0.0042, -0.0322]])\n\n# labels = torch.tensor([1, 1, 1, 1, 3, 0, 2, 1, 3, 1, 1, 3, 0, 0, 1, 3])\n\n# # Initialize the loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # Calculate the cross-entropy loss\n# loss = criterion(predictions, labels)\n\n# print(\"Cross-entropy loss:\", loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:05:08.641856Z","iopub.execute_input":"2024-07-14T05:05:08.642787Z","iopub.status.idle":"2024-07-14T05:05:08.647788Z","shell.execute_reply.started":"2024-07-14T05:05:08.642756Z","shell.execute_reply":"2024-07-14T05:05:08.646714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize the model, criterion, and optimizer\n# model = MultitaskABSA(num_aspects=num_aspects, num_polarities=num_polarities)\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n\n# # Set device to CUDA if available\n# model.to(device)\n\n# # Example of processing one batch\n# for batch in train_loader:\n#     input_ids = batch[0].to(device)\n#     attention_mask = batch[1].to(device)\n#     sentiments = batch[2].to(device)\n#     print(sentiments)\n\n#     optimizer.zero_grad()\n#     outputs = model(input_ids, attention_mask)  # This should be a single tensor now\n#     print(outputs)\n#     loss = 0\n#     for i in range(outputs.shape[1]):  # Iterate over each aspect\n#         loss += criterion(outputs[:, i, :], sentiments[:, i])\n#         print(outputs[:, i, :])\n#         print(sentiments[:, i])\n\n#     # Print the outputs and loss\n#     print(\"Outputs shape:\", outputs.shape)\n#     print(\"Loss:\", loss.item())\n#     break  # Just process one batch for this example\n# tensor([[-0.1922, -0.0503,  0.0264, -0.0277],\n#         [-0.1669, -0.0355, -0.0943, -0.0140],\n#         [-0.0265,  0.0483, -0.0404,  0.0403],\n#         [-0.2612, -0.0051, -0.3087, -0.0231],\n#         [-0.1960,  0.0429, -0.0926,  0.1341],\n#         [-0.0686, -0.0588, -0.2685,  0.0557],\n#         [-0.1588, -0.0055, -0.0791, -0.0144],\n#         [-0.0283, -0.1484, -0.1583,  0.0392],\n#         [-0.3763, -0.0678, -0.0498, -0.0280],\n#         [-0.1223, -0.0743, -0.0192,  0.1066],\n#         [-0.3028,  0.0597, -0.0796, -0.0477],\n#         [-0.0961,  0.0364, -0.0780,  0.0935],\n#         [-0.0869, -0.1123, -0.1555,  0.0233],\n#         [-0.1418,  0.0212, -0.0107, -0.0382],\n#         [-0.0082, -0.1902, -0.1779, -0.0022],\n#         [-0.2244, -0.0243, -0.0601, -0.0686]],","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:05:12.816899Z","iopub.execute_input":"2024-07-14T05:05:12.817629Z","iopub.status.idle":"2024-07-14T05:05:12.823069Z","shell.execute_reply.started":"2024-07-14T05:05:12.817597Z","shell.execute_reply":"2024-07-14T05:05:12.821999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader, TensorDataset\n# from transformers import AutoModel\n\n# # Create a sample dataset\n# batch_size = 16\n# seq_len = 256\n# num_aspects = 10\n# num_polarities = 4\n\n# # Random input data and random sentiment labels\n# input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n# attention_mask = torch.ones((batch_size, seq_len))\n# sentiments = torch.randint(0, num_polarities, (batch_size, num_aspects))\n\n# # Create a DataLoader\n# dataset = TensorDataset(input_ids, attention_mask, sentiments)\n# train_loader = DataLoader(dataset, batch_size=batch_size)\n\n# # Initialize the model, criterion, and optimizer\n# model = MultibranchABSA(num_aspects=num_aspects, num_polarities=num_polarities)\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n\n# # Set device to CUDA if available\n# model.to(device)\n\n# # Example of processing one batch\n# for batch in train_loader:\n#     input_ids = batch[0].to(device)\n#     attention_mask = batch[1].to(device)\n#     sentiments = batch[2].to(device)\n#     print(sentiments)\n\n#     optimizer.zero_grad()\n#     outputs = model(input_ids, attention_mask)  # This should be a single tensor now\n#     print(outputs)\n#     loss = 0\n#     for i in range(outputs.shape[1]):  # Iterate over each aspect\n#         loss += criterion(outputs[:, i, :], sentiments[:, i])\n#         print(outputs[:, i, :])\n#         print(sentiments[:, i])\n\n#     # Print the outputs and loss\n#     print(\"Outputs shape:\", outputs.shape)\n#     print(\"Loss:\", loss.item())\n#     break  # Just process one batch for this example\n#     '''\n#     [-0.0660, -0.0504, -0.1315,  0.0818],\n#         [ 0.0656, -0.0014, -0.1734,  0.0576],\n#         [-0.0932, -0.0394, -0.0433,  0.0731],\n#         [-0.0787, -0.0532, -0.2123,  0.0245],\n#         [-0.0735, -0.0122, -0.1473,  0.0587],\n#         [-0.0469,  0.0221, -0.1044, -0.1553],\n#         [-0.0902, -0.1343, -0.1494, -0.1250],\n#         [-0.0661, -0.0919, -0.0022, -0.2122],\n#         [-0.0051,  0.0075, -0.1667, -0.0824],\n#         [-0.1292, -0.0723,  0.0188, -0.0652],\n#         [-0.1651,  0.0175, -0.1425, -0.0439],\n#         [ 0.0003, -0.0883, -0.1191,  0.2049],\n#         [-0.1044, -0.0775, -0.1074, -0.1481],\n#         [-0.1015, -0.0562, -0.0971,  0.1014],\n#         [ 0.0507, -0.0308, -0.0487, -0.0929],\n#         [-0.0877, -0.0031, -0.1237, -0.1592]], device='cuda:0',\n#        grad_fn=<SliceBackward0>)\n# tensor([3, 1, 3, 2, 3, 2, 1, 0, 0, 0, 0, 3, 2, 2, 3, 0], device='cuda:0')'''","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:05:16.975127Z","iopub.execute_input":"2024-07-14T05:05:16.975507Z","iopub.status.idle":"2024-07-14T05:05:16.982090Z","shell.execute_reply.started":"2024-07-14T05:05:16.975477Z","shell.execute_reply":"2024-07-14T05:05:16.980986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch_optimizer","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:58:01.527218Z","iopub.execute_input":"2024-07-16T03:58:01.528114Z","iopub.status.idle":"2024-07-16T03:58:15.479874Z","shell.execute_reply.started":"2024-07-16T03:58:01.528079Z","shell.execute_reply":"2024-07-16T03:58:15.478666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch_optimizer as optim\nfrom torch.optim import Adam","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:58:15.482029Z","iopub.execute_input":"2024-07-16T03:58:15.482408Z","iopub.status.idle":"2024-07-16T03:58:15.501263Z","shell.execute_reply.started":"2024-07-16T03:58:15.482371Z","shell.execute_reply":"2024-07-16T03:58:15.500295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport time\nimport matplotlib.pyplot as plt\n\ndef train(model, train_loader, val_loader, device, num_epochs=20, patience=5):\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay= 0.01)\n    criterion = nn.CrossEntropyLoss()\n    model.to(device)\n    best_val_loss = float('inf')\n    epochs_without_improvement = 0\n\n    def compute_accuracy(outputs, labels):\n        _, preds = torch.max(outputs, dim=2)\n        corrects = (preds == labels).sum().item()\n        total = labels.numel()\n        accuracy = corrects / total\n        return accuracy\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        model.train()\n        total_loss = 0\n        total_corrects = 0\n        total_samples = 0\n\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            sentiments = batch['sentiments'].to(device)  # Assuming shape (batch_size, num_aspects)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)  # This should be a single tensor now        \n            loss = 0\n            for i in range(outputs.shape[1]):  # Iterate over each aspect\n                loss += criterion(outputs[:, i, :], sentiments[:, i])\n \n                \n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            \n            total_corrects += compute_accuracy(outputs, sentiments) * sentiments.size(0)\n            total_samples += sentiments.size(0)\n        \n        avg_train_loss = total_loss / len(train_loader)\n        avg_train_accuracy = total_corrects / total_samples\n        train_losses.append(avg_train_loss)\n        train_accuracies.append(avg_train_accuracy)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}')\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_corrects = 0\n        val_samples = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                sentiments = batch['sentiments'].to(device)\n\n                outputs = model(input_ids, attention_mask)\n                loss = 0\n                for i in range(outputs.shape[1]):\n                    loss += criterion(outputs[:, i, :], sentiments[:, i])\n                val_loss += loss.item()\n\n                val_corrects += compute_accuracy(outputs, sentiments) * sentiments.size(0)\n                val_samples += sentiments.size(0)\n\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_accuracy = val_corrects / val_samples\n        val_losses.append(avg_val_loss)\n        val_accuracies.append(avg_val_accuracy)\n\n        end_time = time.time()\n        epoch_time = end_time - start_time\n\n        print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}')\n        print(f'Epoch {epoch+1} took {epoch_time:.2f} seconds.')\n        \n        # Early stopping logic\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n            print(f\"No improvement in validation loss for {epochs_without_improvement} epoch(s).\")\n\n            if epochs_without_improvement >= patience:\n                print(f\"Early stopping after {epochs_without_improvement} epochs without improvement.\")\n                break\n\n    return train_losses, val_losses, train_accuracies, val_accuracies, epoch + 1  # Return the actual number of epochs","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:58:15.503000Z","iopub.execute_input":"2024-07-16T03:58:15.503497Z","iopub.status.idle":"2024-07-16T03:58:15.524916Z","shell.execute_reply.started":"2024-07-16T03:58:15.503464Z","shell.execute_reply":"2024-07-16T03:58:15.523799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\n\ndef evaluate(model, data_loader, device, sentiment_to_index):\n    model.eval()\n    aspect_predictions, aspect_true_labels = [], []\n    full_predictions, full_true_labels = [], []\n    combined_aspect_predictions = []\n    combined_aspect_true_labels = []\n    combined_full_predictions = []\n    combined_full_true_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            sentiments = batch['sentiments'].to(device)\n#             print(sentiments)\n\n            outputs = model(input_ids, attention_mask)\n#             print(outputs)\n            logits = outputs[0] if isinstance(outputs, tuple) else outputs  # Make sure to access the logits correctly\n            softmax_logits = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n            # Ensure that the outputs are being indexed correctly\n            if logits.dim() == 3:  # Check if logits are three-dimensional\n                batch_aspect_predictions = []\n                batch_aspect_true_labels = []\n                batch_full_predictions = []\n                batch_full_true_labels = []\n                for i in range(logits.shape[1]):  # Iterate over each aspect\n                    preds = torch.argmax(softmax_logits[:, i, :], dim=1)\n#                     print('pred',preds)\n                    aspect_pred = (preds != sentiment_to_index['None']).long()\n#                     print('pred aspect',aspect_pred)\n                    aspect_true = (sentiments[:, i] != sentiment_to_index['None']).long()\n#                     print('as label',aspect_true)\n                    aspect_predictions.append(aspect_pred.cpu().numpy())\n                    aspect_true_labels.append(aspect_true.cpu().numpy())\n#                     print(aspect_predictions)\n#                     print(aspect_true_labels)\n                    full_predictions.append(preds.cpu().numpy())\n                    full_true_labels.append(sentiments[:, i].cpu().numpy())\n#                     print(full_predictions)\n#                     print(full_true_labels)\n                    batch_aspect_predictions.append(aspect_pred.cpu().numpy())\n                    batch_aspect_true_labels.append(aspect_true.cpu().numpy())\n                    batch_full_predictions.append(preds.cpu().numpy())\n                    batch_full_true_labels.append(sentiments[:, i].cpu().numpy())\n#                     print(batch_aspect_predictions,batch_aspect_true_labels,batch_full_predictions,batch_full_true_labels)\n                # Combine aspect and full predictions/true labels for the current batch\n                for j in range(batch_full_true_labels[0].shape[0]):  # Iterate over the range of batch size\n                    combined_aspect_predictions.append([batch_aspect_predictions[k][j] for k in range(len(batch_aspect_predictions))])\n                    combined_aspect_true_labels.append([batch_aspect_true_labels[k][j] for k in range(len(batch_aspect_true_labels))])\n                    combined_full_predictions.append([batch_full_predictions[k][j] for k in range(len(batch_full_predictions))])\n                    combined_full_true_labels.append([batch_full_true_labels[k][j] for k in range(len(batch_full_true_labels))])\n#                 print(np.array(combined_aspect_predictions),'\\n',np.array(combined_aspect_true_labels),'\\n',np.array(combined_full_predictions),'\\n',np.array(combined_full_true_labels))\n#                 break\n            else:\n                raise ValueError(\"Expected three-dimensional output tensor from model\")\n\n    # Convert combined lists to numpy arrays\n    combined_aspect_predictions = np.array(combined_aspect_predictions)\n    combined_aspect_true_labels = np.array(combined_aspect_true_labels)\n    combined_full_predictions = np.array(combined_full_predictions)\n    combined_full_true_labels = np.array(combined_full_true_labels)\n\n    return combined_aspect_predictions, combined_aspect_true_labels, combined_full_predictions, combined_full_true_labels","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:58:15.527154Z","iopub.execute_input":"2024-07-16T03:58:15.528021Z","iopub.status.idle":"2024-07-16T03:58:15.545347Z","shell.execute_reply.started":"2024-07-16T03:58:15.527982Z","shell.execute_reply":"2024-07-16T03:58:15.544470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_predictions4, aspect_true_labels4, full_predictions4, full_true_labels4 = evaluate(model4, test_loader, device, sentiment_to_index) #2e5","metadata":{"execution":{"iopub.status.busy":"2024-07-14T18:38:50.913323Z","iopub.execute_input":"2024-07-14T18:38:50.914211Z","iopub.status.idle":"2024-07-14T18:38:51.090124Z","shell.execute_reply.started":"2024-07-14T18:38:50.914161Z","shell.execute_reply":"2024-07-14T18:38:51.088987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metrics(train_metrics_concat, val_metrics_concat, train_metrics_multibranch, val_metrics_multibranch, actual_epochs_concat, actual_epochs_multibranch):\n    max_epochs = max(actual_epochs_concat, actual_epochs_multibranch)\n    epochs_range_concat = range(1, actual_epochs_concat + 1)\n    epochs_range_multibranch = range(1, actual_epochs_multibranch + 1)\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n    \n    # Plot loss\n    ax1.plot(epochs_range_concat, train_metrics_concat['loss'], label='Train Loss - MultiTask', color='blue')\n    ax1.plot(epochs_range_concat, val_metrics_concat['loss'], label='Val Loss - MultiTask', color='green')\n    ax1.plot(epochs_range_multibranch, train_metrics_multibranch['loss'], label='Train Loss - MultiBranch', color='red')\n    ax1.plot(epochs_range_multibranch, val_metrics_multibranch['loss'], label='Val Loss - AMultiBranch', color='orange')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Loss Over Time')\n    ax1.legend()\n    \n    # Plot accuracy\n    ax2.plot(epochs_range_concat, train_metrics_concat['accuracy'], label='Train Accuracy - MultiTask', color='blue')\n    ax2.plot(epochs_range_concat, val_metrics_concat['accuracy'], label='Val Accuracy - MultiTask', color='green')\n    ax2.plot(epochs_range_multibranch, train_metrics_multibranch['accuracy'], label='Train Accuracy - MultiBranch', color='red')\n    ax2.plot(epochs_range_multibranch, val_metrics_multibranch['accuracy'], label='Val Accuracy - MultiBranch', color='orange')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Accuracy Over Time')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T02:58:27.165781Z","iopub.execute_input":"2024-07-15T02:58:27.166428Z","iopub.status.idle":"2024-07-15T02:58:27.177238Z","shell.execute_reply.started":"2024-07-15T02:58:27.166393Z","shell.execute_reply":"2024-07-15T02:58:27.176242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_concat = MultiTaskABSAConcat(n_aspects, n_labels).to(device)\n# model_multibranch = MultiTaskABSAMultiBranch(n_aspects, n_labels).to(device)\n\n# # Train and evaluate both models\n# train_metrics_concat = {'loss': [], 'accuracy': []}\n# val_metrics_concat = {'loss': [], 'accuracy': []}\n# train_metrics_multibranch = {'loss': [], 'accuracy': []}\n# val_metrics_multibranch = {'loss': [], 'accuracy': []}\n\n# train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs_concat = train(model_concat, dataloader, val_dataloader, device, num_epochs=20, patience=5)\n# train_metrics_concat['loss'] = train_losses\n# val_metrics_concat['loss'] = val_losses\n# train_metrics_concat['accuracy'] = train_accuracies\n# val_metrics_concat['accuracy'] = val_accuracies\n\n# train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs_multibranch = train(model_multibranch, dataloader, val_dataloader, device, num_epochs=20, patience=5)\n# train_metrics_multibranch['loss'] = train_losses\n# val_metrics_multibranch['loss'] = val_losses\n# train_metrics_multibranch['accuracy'] = train_accuracies\n# val_metrics_multibranch['accuracy'] = val_accuracies\n\n# # Plot metrics\n# plot_metrics(train_metrics_concat, val_metrics_concat, train_metrics_multibranch, val_metrics_multibranch, actual_epochs_concat, actual_epochs_multibranch)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T21:42:46.902778Z","iopub.execute_input":"2024-06-24T21:42:46.903452Z","iopub.status.idle":"2024-06-24T21:42:46.908415Z","shell.execute_reply.started":"2024-06-24T21:42:46.903414Z","shell.execute_reply":"2024-06-24T21:42:46.907549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model1 =MultibranchABSA().to(device)\n# model2=MultitaskABSA().to(device)\n# # model3=NewApproach().to(device)\nmodel4=MultitaskABSAConcat().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:59:59.792257Z","iopub.execute_input":"2024-07-16T03:59:59.793010Z","iopub.status.idle":"2024-07-16T04:00:00.300789Z","shell.execute_reply.started":"2024-07-16T03:59:59.792976Z","shell.execute_reply":"2024-07-16T04:00:00.299616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate both models\ntrain_metrics_concat = {'loss': [], 'accuracy': []}\nval_metrics_concat = {'loss': [], 'accuracy': []}\ntrain_metrics_multibranch = {'loss': [], 'accuracy': []}\nval_metrics_multibranch = {'loss': [], 'accuracy': []}","metadata":{"execution":{"iopub.status.busy":"2024-07-15T02:57:33.427644Z","iopub.execute_input":"2024-07-15T02:57:33.428315Z","iopub.status.idle":"2024-07-15T02:57:33.434003Z","shell.execute_reply.started":"2024-07-15T02:57:33.428281Z","shell.execute_reply":"2024-07-15T02:57:33.432851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model4=MultitaskABSAConcat().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T17:21:54.411315Z","iopub.execute_input":"2024-07-08T17:21:54.412079Z","iopub.status.idle":"2024-07-08T17:21:54.975399Z","shell.execute_reply.started":"2024-07-08T17:21:54.412050Z","shell.execute_reply":"2024-07-08T17:21:54.974380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs_concat = train(model4, train_loader, val_loader, device, num_epochs=20, patience=5)\ntrain_metrics_concat['loss'] = train_losses\nval_metrics_concat['loss'] = val_losses\ntrain_metrics_concat['accuracy'] = train_accuracies\nval_metrics_concat['accuracy'] = val_accuracies\n# Plot metrics\n# plot_metrics(train_metrics_concat, val_metrics_concat, train_metrics_multibranch, val_metrics_multibranch, actual_epochs_concat, actual_epochs_multibranch)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T03:38:46.374604Z","iopub.execute_input":"2024-07-15T03:38:46.375699Z","iopub.status.idle":"2024-07-15T04:15:45.921440Z","shell.execute_reply.started":"2024-07-15T03:38:46.375657Z","shell.execute_reply":"2024-07-15T04:15:45.920275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_predictions4, aspect_true_labels4, full_predictions4, full_true_labels4 = evaluate(model4, test_loader, device, sentiment_to_index) #2e5","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:40:22.440379Z","iopub.execute_input":"2024-07-09T13:40:22.441463Z","iopub.status.idle":"2024-07-09T13:40:22.639480Z","shell.execute_reply.started":"2024-07-09T13:40:22.441429Z","shell.execute_reply":"2024-07-09T13:40:22.638207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs_multibranch = train(model1, train_loader, val_loader, device, num_epochs=20, patience=5)\n\ntrain_metrics_multibranch['loss'] = train_losses\nval_metrics_multibranch['loss'] = val_losses\ntrain_metrics_multibranch['accuracy'] = train_accuracies\nval_metrics_multibranch['accuracy'] = val_accuracies","metadata":{"execution":{"iopub.status.busy":"2024-07-15T02:59:07.903204Z","iopub.execute_input":"2024-07-15T02:59:07.904049Z","iopub.status.idle":"2024-07-15T03:36:06.653160Z","shell.execute_reply.started":"2024-07-15T02:59:07.904015Z","shell.execute_reply":"2024-07-15T03:36:06.652163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_predictions1, aspect_true_labels1, full_predictions1, full_true_labels1 = evaluate(model1, test_loader, device, sentiment_to_index) #2e5","metadata":{"execution":{"iopub.status.busy":"2024-07-09T12:21:19.921616Z","iopub.execute_input":"2024-07-09T12:21:19.922542Z","iopub.status.idle":"2024-07-09T12:21:38.485782Z","shell.execute_reply.started":"2024-07-09T12:21:19.922501Z","shell.execute_reply":"2024-07-09T12:21:38.484793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(train_metrics_concat, val_metrics_concat, train_metrics_multibranch, val_metrics_multibranch, actual_epochs_concat, actual_epochs_multibranch)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T04:36:00.876431Z","iopub.execute_input":"2024-07-15T04:36:00.876834Z","iopub.status.idle":"2024-07-15T04:36:01.557523Z","shell.execute_reply.started":"2024-07-15T04:36:00.876803Z","shell.execute_reply":"2024-07-15T04:36:01.556608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_names = list(aspect_to_index.keys())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:46:49.821017Z","iopub.execute_input":"2024-07-16T01:46:49.822031Z","iopub.status.idle":"2024-07-16T01:46:49.826186Z","shell.execute_reply.started":"2024-07-16T01:46:49.821997Z","shell.execute_reply":"2024-07-16T01:46:49.825145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_predictions4, aspect_true_labels4, full_predictions4, full_true_labels4 = evaluate(model4, test_loader, device, sentiment_to_index) #1E5","metadata":{"execution":{"iopub.status.busy":"2024-07-15T04:48:50.100922Z","iopub.execute_input":"2024-07-15T04:48:50.101297Z","iopub.status.idle":"2024-07-15T04:49:08.800125Z","shell.execute_reply.started":"2024-07-15T04:48:50.101268Z","shell.execute_reply":"2024-07-15T04:49:08.799155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_predictions1, aspect_true_labels1, full_predictions1, full_true_labels1 = evaluate(model1, test_loader, device, sentiment_to_index) #1E5","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:03:10.628635Z","iopub.execute_input":"2024-07-15T05:03:10.629469Z","iopub.status.idle":"2024-07-15T05:03:29.268471Z","shell.execute_reply.started":"2024-07-15T05:03:10.629439Z","shell.execute_reply":"2024-07-15T05:03:29.267434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = polar_metrics(full_true_labels1, full_predictions1, aspect_names)\nprint(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:03:29.270275Z","iopub.execute_input":"2024-07-15T05:03:29.270645Z","iopub.status.idle":"2024-07-15T05:03:29.332400Z","shell.execute_reply.started":"2024-07-15T05:03:29.270612Z","shell.execute_reply":"2024-07-15T05:03:29.331481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = polar_metrics_none(full_true_labels1, full_predictions1, aspect_names)\nprint(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:04:12.717456Z","iopub.execute_input":"2024-07-15T05:04:12.718317Z","iopub.status.idle":"2024-07-15T05:04:12.778074Z","shell.execute_reply.started":"2024-07-15T05:04:12.718284Z","shell.execute_reply":"2024-07-15T05:04:12.777062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = aspect_detection_metrics(aspect_true_labels1, aspect_predictions1, aspect_names)\nprint(report)#sche 3e5","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:04:22.014339Z","iopub.execute_input":"2024-07-15T05:04:22.014703Z","iopub.status.idle":"2024-07-15T05:04:22.119413Z","shell.execute_reply.started":"2024-07-15T05:04:22.014658Z","shell.execute_reply":"2024-07-15T05:04:22.118463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = polar_metrics(full_true_labels4, full_predictions4, aspect_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:04:39.793934Z","iopub.execute_input":"2024-07-15T05:04:39.794754Z","iopub.status.idle":"2024-07-15T05:04:39.856244Z","shell.execute_reply.started":"2024-07-15T05:04:39.794716Z","shell.execute_reply":"2024-07-15T05:04:39.855269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = polar_metrics_none(full_true_labels4, full_predictions4, aspect_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:04:26.554598Z","iopub.execute_input":"2024-07-15T05:04:26.555397Z","iopub.status.idle":"2024-07-15T05:04:26.611630Z","shell.execute_reply.started":"2024-07-15T05:04:26.555367Z","shell.execute_reply":"2024-07-15T05:04:26.610726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = aspect_detection_metrics(aspect_true_labels4, aspect_predictions4, aspect_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T05:05:00.915354Z","iopub.execute_input":"2024-07-15T05:05:00.915967Z","iopub.status.idle":"2024-07-15T05:05:01.020191Z","shell.execute_reply.started":"2024-07-15T05:05:00.915938Z","shell.execute_reply":"2024-07-15T05:05:01.019245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs_multitask = train(model2, train_loader, val_loader, device, num_epochs=20, patience=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T01:47:54.757055Z","iopub.execute_input":"2024-07-16T01:47:54.757438Z","iopub.status.idle":"2024-07-16T02:21:35.112648Z","shell.execute_reply.started":"2024-07-16T01:47:54.757408Z","shell.execute_reply":"2024-07-16T02:21:35.111648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_predictions2, aspect_true_labels2, full_predictions2, full_true_labels2 = evaluate(model2, test_loader, device, sentiment_to_index) #1E5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T02:44:13.692432Z","iopub.execute_input":"2024-07-16T02:44:13.692808Z","iopub.status.idle":"2024-07-16T02:44:32.332944Z","shell.execute_reply.started":"2024-07-16T02:44:13.692781Z","shell.execute_reply":"2024-07-16T02:44:32.332121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses,val_losses)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:00:40.993145Z","iopub.execute_input":"2024-07-16T03:00:40.994211Z","iopub.status.idle":"2024-07-16T03:00:41.271504Z","shell.execute_reply.started":"2024-07-16T03:00:40.994170Z","shell.execute_reply":"2024-07-16T03:00:41.270546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs_multibranch = train(model1, train_loader, val_loader, device, num_epochs=20, patience=5)\n# aspect_predictions1, aspect_true_labels1, full_predictions1, full_true_labels1 = evaluate(model1, test_loader, device, sentiment_to_index) #2e5\n\n# train_metrics_multibranch['loss'] = train_losses\n# val_metrics_multibranch['loss'] = val_losses\n# train_metrics_multibranch['accuracy'] = train_accuracies\n# val_metrics_multibranch['accuracy'] = val_accuracies","metadata":{"execution":{"iopub.status.busy":"2024-07-08T15:06:58.739099Z","iopub.execute_input":"2024-07-08T15:06:58.739797Z","iopub.status.idle":"2024-07-08T15:06:59.029328Z","shell.execute_reply.started":"2024-07-08T15:06:58.739760Z","shell.execute_reply":"2024-07-08T15:06:59.028053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:30:48.538590Z","iopub.execute_input":"2024-07-07T15:30:48.539670Z","iopub.status.idle":"2024-07-07T15:30:48.543586Z","shell.execute_reply.started":"2024-07-07T15:30:48.539636Z","shell.execute_reply":"2024-07-07T15:30:48.542596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_metrics(train_metrics_concat, val_metrics_concat, train_metrics_multibranch, val_metrics_multibranch, actual_epochs_concat, actual_epochs_multibranch)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T16:34:17.337260Z","iopub.execute_input":"2024-07-07T16:34:17.337637Z","iopub.status.idle":"2024-07-07T16:34:17.986638Z","shell.execute_reply.started":"2024-07-07T16:34:17.337609Z","shell.execute_reply":"2024-07-07T16:34:17.985660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = polar_metrics_none(full_true_labels2, full_predictions2, aspect_names)\nprint(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:01:21.749733Z","iopub.execute_input":"2024-07-16T03:01:21.750312Z","iopub.status.idle":"2024-07-16T03:01:21.807878Z","shell.execute_reply.started":"2024-07-16T03:01:21.750282Z","shell.execute_reply":"2024-07-16T03:01:21.806939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = polar_metrics(full_true_labels2, full_predictions2, aspect_names)\nprint(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:01:35.955124Z","iopub.execute_input":"2024-07-16T03:01:35.955970Z","iopub.status.idle":"2024-07-16T03:01:36.016280Z","shell.execute_reply.started":"2024-07-16T03:01:35.955940Z","shell.execute_reply":"2024-07-16T03:01:36.015373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = aspect_detection_metrics(aspect_true_labels2, aspect_predictions2, aspect_names)\nprint(report)#sche 3e5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:01:53.388081Z","iopub.execute_input":"2024-07-16T03:01:53.388936Z","iopub.status.idle":"2024-07-16T03:01:53.493956Z","shell.execute_reply.started":"2024-07-16T03:01:53.388907Z","shell.execute_reply":"2024-07-16T03:01:53.493033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example usage:\n# report = aspect_detection_metrics(aspect_true_labels4, aspect_predictions4, aspect_names)\n# print(report)#sche 3e5","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:47:12.393657Z","iopub.execute_input":"2024-06-30T07:47:12.394014Z","iopub.status.idle":"2024-06-30T07:47:12.499563Z","shell.execute_reply.started":"2024-06-30T07:47:12.393985Z","shell.execute_reply":"2024-06-30T07:47:12.498687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_losses(train_losses, val_losses):\n    import matplotlib.pyplot as plt\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Train Loss', color='red')\n    plt.plot(val_losses, label='Validation Loss', color='blue')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:00:07.714724Z","iopub.execute_input":"2024-07-16T03:00:07.715321Z","iopub.status.idle":"2024-07-16T03:00:07.721043Z","shell.execute_reply.started":"2024-07-16T03:00:07.715291Z","shell.execute_reply":"2024-07-16T03:00:07.720150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_to_index","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:00:54.129653Z","iopub.execute_input":"2024-07-16T03:00:54.130026Z","iopub.status.idle":"2024-07-16T03:00:54.136396Z","shell.execute_reply.started":"2024-07-16T03:00:54.129997Z","shell.execute_reply":"2024-07-16T03:00:54.135506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_names = list(aspect_to_index.keys())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:00:55.083791Z","iopub.execute_input":"2024-07-16T03:00:55.084400Z","iopub.status.idle":"2024-07-16T03:00:55.088570Z","shell.execute_reply.started":"2024-07-16T03:00:55.084370Z","shell.execute_reply":"2024-07-16T03:00:55.087639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for aspect_idx, aspect_name in enumerate(aspect_names):\n    print(aspect_idx,aspect_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:31:00.265864Z","iopub.execute_input":"2024-07-07T15:31:00.266249Z","iopub.status.idle":"2024-07-07T15:31:00.271978Z","shell.execute_reply.started":"2024-07-07T15:31:00.266212Z","shell.execute_reply":"2024-07-07T15:31:00.271139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef polar_metrics_none(full_true_labels, full_predictions, aspect_names):\n    aspect_to_index = {\n        'Negative': 0, \n        'Positive': 1, \n        'Neutral': 2\n    }\n    total_macro_precision = 0\n    total_macro_recall = 0\n    total_macro_f1 = 0\n    total_macro_support = 0\n    total_acc = 0\n    \n    num_aspects = len(aspect_names)\n    # Initialize report\n    report = \"Aspect + Polarity Metrics:\\n\"\n    \n    # Generate metrics for each aspect\n    for aspect_idx, aspect_name in enumerate(aspect_names):\n        sentiment_labels = full_true_labels[:, aspect_idx]\n        sentiment_predictions = full_predictions[:, aspect_idx]\n\n        # Filter out 'None' sentiment\n        valid_indices = sentiment_labels != 3\n        sentiment_labels = sentiment_labels[valid_indices]\n        sentiment_predictions = sentiment_predictions[valid_indices]\n    \n        precision, recall, fscore, support = precision_recall_fscore_support(\n            sentiment_labels, sentiment_predictions, labels=[0, 1, 2], zero_division=1\n        )\n        \n        macro_avg = precision_recall_fscore_support(\n            sentiment_labels, sentiment_predictions, average='macro', zero_division=1\n        )\n        weighted_avg = precision_recall_fscore_support(\n           sentiment_labels, sentiment_predictions, average='weighted', zero_division=1\n        )\n        accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n        \n        total_macro_precision += macro_avg[0]\n        total_macro_recall += macro_avg[1]\n        total_macro_f1 += macro_avg[2]\n        total_macro_support += np.sum(support)\n        total_acc += accuracy\n        report += f\"Metrics for {aspect_name}:\\n\"\n        report += f\"{'Polarity':<15} {'precision':<10} {'recall':<10} {'f1-score':<10} {'support':<10}\\n\"\n    \n        for polarity_name, i in aspect_to_index.items():\n            report += f\"{polarity_name:<15} {precision[i]:<10.2f} {recall[i]:<10.2f} {fscore[i]:<10.2f} {support[i]:<10}\\n\"\n    \n        report += (\n            f\"\\n{'accuracy':<15} {accuracy:<10.2f} {'':<10} {'':<10} {np.sum(support):<10}\\n\"\n            f\"{'macro avg':<15} {macro_avg[0]:<10.2f} {macro_avg[1]:<10.2f} {macro_avg[2]:<10.2f} {np.sum(support):<10}\\n\"\n            f\"{'weighted avg':<15} {weighted_avg[0]:<10.2f} {weighted_avg[1]:<10.2f} {weighted_avg[2]:<10.2f} {np.sum(support):<10}\\n\"\n            f\"{'-' * 60}\\n\"\n        )\n    # Calculate overall macro averages\n    overall_macro_precision = total_macro_precision / num_aspects\n    overall_macro_recall = total_macro_recall / num_aspects\n    overall_macro_f1 = total_macro_f1 / num_aspects\n    overall_acc = total_acc / num_aspects\n    \n    report += \"\\nOverall Macro Averages:\\n\"\n    report += f\"  Precision: {overall_macro_precision:.4f}, Recall: {overall_macro_recall:.4f}, F1-score: {overall_macro_f1:.4f}, Accuracy: {overall_acc:.4f}\\n\"\n    \n    return report\n\n# Example usage:\n# report = polar_metrics_none(full_true_labels, full_predictions, aspect_names)\n# print(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:39:31.699653Z","iopub.execute_input":"2024-07-16T04:39:31.700054Z","iopub.status.idle":"2024-07-16T04:39:31.713260Z","shell.execute_reply.started":"2024-07-16T04:39:31.700026Z","shell.execute_reply":"2024-07-16T04:39:31.712381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef polar_metrics(full_true_labels, full_predictions, aspect_names):\n    aspect_to_index = {\n        'Negative': 0, \n        'Positive': 1, \n        'Neutral': 2,\n        'None': 3\n    }\n    total_macro_precision = 0\n    total_macro_recall = 0\n    total_macro_f1 = 0\n    total_macro_support = 0\n    total_acc = 0\n    \n    num_aspects = len(aspect_names)\n    # Initialize report\n    report = \"Aspect + Polarity Metrics:\\n\"\n    \n    # Generate metrics for each aspect\n    for aspect_idx, aspect_name in enumerate(aspect_names):\n        sentiment_labels = full_true_labels[:, aspect_idx]\n        sentiment_predictions = full_predictions[:, aspect_idx]\n    \n        precision, recall, fscore, support = precision_recall_fscore_support(\n            sentiment_labels, sentiment_predictions, labels=[0, 1, 2, 3], zero_division=1\n        )\n        \n        macro_avg = precision_recall_fscore_support(\n            sentiment_labels, sentiment_predictions, average='macro', zero_division=1\n        )\n        weighted_avg = precision_recall_fscore_support(\n           sentiment_labels, sentiment_predictions, average='weighted', zero_division=1\n        )\n        accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n        \n        total_macro_precision += macro_avg[0]\n        total_macro_recall += macro_avg[1]\n        total_macro_f1 += macro_avg[2]\n        total_macro_support += np.sum(support)\n        total_acc += accuracy\n        report += f\"Metrics for {aspect_name}:\\n\"\n        report += f\"{'Polarity':<15} {'precision':<10} {'recall':<10} {'f1-score':<10} {'support':<10}\\n\"\n    \n        for polarity_name, i in aspect_to_index.items():\n            report += f\"{polarity_name:<15} {precision[i]:<10.2f} {recall[i]:<10.2f} {fscore[i]:<10.2f} {support[i]:<10}\\n\"\n    \n        report += (\n            f\"\\n{'accuracy':<15} {accuracy:<10.2f} {'':<10} {'':<10} {np.sum(support):<10}\\n\"\n            f\"{'macro avg':<15} {macro_avg[0]:<10.2f} {macro_avg[1]:<10.2f} {macro_avg[2]:<10.2f} {np.sum(support):<10}\\n\"\n            f\"{'weighted avg':<15} {weighted_avg[0]:<10.2f} {weighted_avg[1]:<10.2f} {weighted_avg[2]:<10.2f} {np.sum(support):<10}\\n\"\n            f\"{'-' * 60}\\n\"\n        )\n    # Calculate overall macro averages\n    overall_macro_precision = total_macro_precision / num_aspects\n    overall_macro_recall = total_macro_recall / num_aspects\n    overall_macro_f1 = total_macro_f1 / num_aspects\n    overall_acc = total_acc / num_aspects\n    \n    report += \"\\nOverall Macro Averages:\\n\"\n    report += f\"  Precision: {overall_macro_precision:.4f}, Recall: {overall_macro_recall:.4f}, F1-score: {overall_macro_f1:.4f}, Accuracy: {overall_acc:.4f}\\n\"\n    \n    return report\n\n# report = polar_metrics(full_true_labels, full_predictions, aspect_names)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:39:32.302872Z","iopub.execute_input":"2024-07-16T04:39:32.303511Z","iopub.status.idle":"2024-07-16T04:39:32.316319Z","shell.execute_reply.started":"2024-07-16T04:39:32.303478Z","shell.execute_reply":"2024-07-16T04:39:32.315428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\ndef aspect_detection_metrics(aspect_true_labels, aspect_predictions, aspect_names):\n    report = \"Aspect Detection Metrics:\\n\"\n\n    # Initialize accumulators for macro averages\n    total_precision = 0\n    total_recall = 0\n    total_f1_score = 0\n    total_accuracy = 0\n    num_aspects = len(aspect_names)\n\n    # Generate metrics for each aspect\n    for aspect_idx, aspect_name in enumerate(aspect_names):\n        aspect_true = aspect_true_labels[:, aspect_idx]\n        aspect_pred = aspect_predictions[:, aspect_idx]\n\n        aspect_report_dict = classification_report(\n            aspect_true, aspect_pred, digits=4, zero_division=1, output_dict=True\n        )\n\n        precision = aspect_report_dict[\"1\"][\"precision\"]\n        recall = aspect_report_dict[\"1\"][\"recall\"]\n        f1_score = aspect_report_dict[\"1\"][\"f1-score\"]\n        accuracy = accuracy_score(aspect_true, aspect_pred)\n\n        # Accumulate macro averages\n        total_precision += precision\n        total_recall += recall\n        total_f1_score += f1_score\n        total_accuracy += accuracy\n\n        # Add aspect metrics to the report\n        report += f\"  {aspect_name} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}, Accuracy: {accuracy:.4f}\\n\"\n\n    # Calculate overall macro averages\n    overall_macro_precision = total_precision / num_aspects\n    overall_macro_recall = total_recall / num_aspects\n    overall_macro_f1_score = total_f1_score / num_aspects\n    overall_macro_accuracy = total_accuracy / num_aspects\n\n    # Add overall macro averages to the report\n    report += \"\\nOverall Macro Averages:\\n\"\n    report += f\"  Precision: {overall_macro_precision:.4f}, Recall: {overall_macro_recall:.4f}, F1-score: {overall_macro_f1_score:.4f}, Accuracy: {overall_macro_accuracy:.4f}\\n\"\n\n    return report","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:39:33.235227Z","iopub.execute_input":"2024-07-16T04:39:33.235620Z","iopub.status.idle":"2024-07-16T04:39:33.246443Z","shell.execute_reply.started":"2024-07-16T04:39:33.235576Z","shell.execute_reply":"2024-07-16T04:39:33.245425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model1 =MultibranchABSA().to(device)\n# model2=MultitaskABSA().to(device)\n# model3=NewApproach().to(device)\n# model4=MultitaskABSAConcat().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T07:47:51.106156Z","iopub.execute_input":"2024-06-30T07:47:51.106736Z","iopub.status.idle":"2024-06-30T07:47:51.612488Z","shell.execute_reply.started":"2024-06-30T07:47:51.106705Z","shell.execute_reply":"2024-06-30T07:47:51.611677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = NewApproach()\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T02:23:45.698961Z","iopub.execute_input":"2024-06-30T02:23:45.699580Z","iopub.status.idle":"2024-06-30T02:23:45.703630Z","shell.execute_reply.started":"2024-06-30T02:23:45.699546Z","shell.execute_reply":"2024-06-30T02:23:45.702702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# aspect overall","metadata":{}},{"cell_type":"code","source":"aspect_names","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:40:28.783735Z","iopub.execute_input":"2024-07-16T04:40:28.784133Z","iopub.status.idle":"2024-07-16T04:40:28.790900Z","shell.execute_reply.started":"2024-07-16T04:40:28.784106Z","shell.execute_reply":"2024-07-16T04:40:28.789995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aspect_name1= ['GENERAL',\n 'SER&ACC',\n 'SCREEN',\n 'CAMERA',\n 'FEATURES',\n 'BATTERY',\n 'PERFORMANCE',\n 'STORAGE',\n 'DESIGN',\n 'PRICE']","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:40:46.057369Z","iopub.execute_input":"2024-07-16T04:40:46.058325Z","iopub.status.idle":"2024-07-16T04:40:46.063836Z","shell.execute_reply.started":"2024-07-16T04:40:46.058291Z","shell.execute_reply":"2024-07-16T04:40:46.062802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses, train_accuracies, val_accuracies, actual_epochs = train(model4, train_loader, val_loader, device, num_epochs=20, patience=5)\naspect_predictions, aspect_true_labels, full_predictions, full_true_labels = evaluate(model4, test_loader, device, sentiment_to_index) #2e5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:00:19.832700Z","iopub.execute_input":"2024-07-16T04:00:19.833087Z","iopub.status.idle":"2024-07-16T04:37:45.869086Z","shell.execute_reply.started":"2024-07-16T04:00:19.833058Z","shell.execute_reply":"2024-07-16T04:37:45.868239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_predictions, aspect_true_labels, full_predictions, full_true_labels = evaluate(model4, test_loader, device, sentiment_to_index) #1E5","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:31:40.502965Z","iopub.execute_input":"2024-07-07T15:31:40.503361Z","iopub.status.idle":"2024-07-07T15:31:58.973432Z","shell.execute_reply.started":"2024-07-07T15:31:40.503331Z","shell.execute_reply":"2024-07-07T15:31:58.972412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = polar_metrics(full_true_labels, full_predictions, aspect_name1)\nprint(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:41:06.492695Z","iopub.execute_input":"2024-07-16T04:41:06.493120Z","iopub.status.idle":"2024-07-16T04:41:06.563100Z","shell.execute_reply.started":"2024-07-16T04:41:06.493088Z","shell.execute_reply":"2024-07-16T04:41:06.561958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = polar_metrics(full_true_labels, full_predictions, aspect_names)\nprint(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:39:36.448536Z","iopub.execute_input":"2024-07-16T04:39:36.448951Z","iopub.status.idle":"2024-07-16T04:39:36.530797Z","shell.execute_reply.started":"2024-07-16T04:39:36.448925Z","shell.execute_reply":"2024-07-16T04:39:36.529661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = aspect_detection_metrics(aspect_true_labels, aspect_predictions, aspect_name1)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:41:20.234868Z","iopub.execute_input":"2024-07-16T04:41:20.235703Z","iopub.status.idle":"2024-07-16T04:41:20.349966Z","shell.execute_reply.started":"2024-07-16T04:41:20.235669Z","shell.execute_reply":"2024-07-16T04:41:20.348956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nreport = aspect_detection_metrics(aspect_true_labels, aspect_predictions, aspect_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:39:40.399972Z","iopub.execute_input":"2024-07-16T04:39:40.400349Z","iopub.status.idle":"2024-07-16T04:39:40.531746Z","shell.execute_reply.started":"2024-07-16T04:39:40.400323Z","shell.execute_reply":"2024-07-16T04:39:40.530665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = polar_metrics_none(full_true_labels, full_predictions, aspect_name1)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:41:30.262778Z","iopub.execute_input":"2024-07-16T04:41:30.263461Z","iopub.status.idle":"2024-07-16T04:41:30.325387Z","shell.execute_reply.started":"2024-07-16T04:41:30.263428Z","shell.execute_reply":"2024-07-16T04:41:30.324369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = polar_metrics_none(full_true_labels, full_predictions, aspect_names)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:39:42.602880Z","iopub.execute_input":"2024-07-16T04:39:42.603262Z","iopub.status.idle":"2024-07-16T04:39:42.676131Z","shell.execute_reply.started":"2024-07-16T04:39:42.603231Z","shell.execute_reply":"2024-07-16T04:39:42.675146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example usage:\n# report = aspect_detection_metrics(aspect_true_labels1, aspect_predictions1, aspect_names)\n# print(report)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T16:28:50.568459Z","iopub.execute_input":"2024-07-07T16:28:50.568811Z","iopub.status.idle":"2024-07-07T16:28:50.676281Z","shell.execute_reply.started":"2024-07-07T16:28:50.568783Z","shell.execute_reply":"2024-07-07T16:28:50.675246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_predictions, aspect_true_labels, full_predictions, full_true_labels = evaluate(model1, test_loader, device, sentiment_to_index) #1E5","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:07:04.270196Z","iopub.execute_input":"2024-06-26T05:07:04.271284Z","iopub.status.idle":"2024-06-26T05:07:24.252858Z","shell.execute_reply.started":"2024-06-26T05:07:04.271247Z","shell.execute_reply":"2024-06-26T05:07:24.252049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model3.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model4.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import  get_linear_schedule_with_warmup\n# optimizer4 = torch.optim.AdamW(model4.parameters(), lr=1e-5, weight_decay=0.01)\n# total_steps = len(train_loader) * 20  # Assuming 20 epochs\n# scheduler4 = get_linear_schedule_with_warmup(optimizer4, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:35:03.924806Z","iopub.execute_input":"2024-06-24T20:35:03.925205Z","iopub.status.idle":"2024-06-24T20:35:03.932613Z","shell.execute_reply.started":"2024-06-24T20:35:03.925163Z","shell.execute_reply":"2024-06-24T20:35:03.93172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_steps","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:03:02.853713Z","iopub.execute_input":"2024-06-20T23:03:02.854672Z","iopub.status.idle":"2024-06-20T23:03:02.858926Z","shell.execute_reply.started":"2024-06-20T23:03:02.854637Z","shell.execute_reply":"2024-06-20T23:03:02.857769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AdamW, get_linear_schedule_with_warmup\n# optimizer1 = AdamW(model1.parameters(), lr=1e-5)\n# total_steps = len(train_loader) * 20  # Assuming 20 epochs\n# scheduler1 = get_linear_schedule_with_warmup(optimizer1, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:03:06.381484Z","iopub.execute_input":"2024-06-20T23:03:06.38202Z","iopub.status.idle":"2024-06-20T23:03:06.385998Z","shell.execute_reply.started":"2024-06-20T23:03:06.381978Z","shell.execute_reply":"2024-06-20T23:03:06.38506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer2 = AdamW(model2.parameters(), lr=2e-5)\n# scheduler2 = get_linear_schedule_with_warmup(optimizer2, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:03:09.416517Z","iopub.execute_input":"2024-06-20T23:03:09.416882Z","iopub.status.idle":"2024-06-20T23:03:09.421125Z","shell.execute_reply.started":"2024-06-20T23:03:09.416853Z","shell.execute_reply":"2024-06-20T23:03:09.420077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AdamW, get_linear_schedule_with_warmup\n# total_steps = len(train_loader) * 20  # Assuming 20 epochs\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:03:12.291143Z","iopub.execute_input":"2024-06-20T23:03:12.291483Z","iopub.status.idle":"2024-06-20T23:03:12.295995Z","shell.execute_reply.started":"2024-06-20T23:03:12.291457Z","shell.execute_reply":"2024-06-20T23:03:12.294803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer3 = AdamW(model3.parameters(), lr=1e-5, weight_decay=0.01)\n# scheduler3 = get_linear_schedule_with_warmup(optimizer3, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer4 = AdamW(model4.parameters(), lr=2e-5, weight_decay=0.01 )\n# scheduler4 = get_linear_schedule_with_warmup(optimizer4, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:03:21.261508Z","iopub.execute_input":"2024-06-20T23:03:21.262001Z","iopub.status.idle":"2024-06-20T23:03:21.266554Z","shell.execute_reply.started":"2024-06-20T23:03:21.261967Z","shell.execute_reply":"2024-06-20T23:03:21.265442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### MultibranchABSA","metadata":{}},{"cell_type":"code","source":"# plot_losses(train_losses, val_losses)#sche e5\n# model = MultitaskABSAConcat()\n# model.to(device)\n# model.load_state_dict(torch.load('/kaggle/working/best_model2.pth'))\n# aspect_predictions, aspect_true_labels, full_predictions, full_true_labels = evaluate(model, test_loader, device, sentiment_to_index) #2e5","metadata":{"execution":{"iopub.status.busy":"2024-06-24T21:42:53.440558Z","iopub.execute_input":"2024-06-24T21:42:53.441262Z","iopub.status.idle":"2024-06-24T21:43:13.02093Z","shell.execute_reply.started":"2024-06-24T21:42:53.441217Z","shell.execute_reply":"2024-06-24T21:43:13.019894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_names = list(aspect_to_index.keys())","metadata":{"execution":{"iopub.status.busy":"2024-07-08T16:35:10.983803Z","iopub.execute_input":"2024-07-08T16:35:10.984619Z","iopub.status.idle":"2024-07-08T16:35:10.988711Z","shell.execute_reply.started":"2024-07-08T16:35:10.984588Z","shell.execute_reply":"2024-07-08T16:35:10.987673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aspect_names","metadata":{"execution":{"iopub.status.busy":"2024-07-08T16:35:17.871265Z","iopub.execute_input":"2024-07-08T16:35:17.871644Z","iopub.status.idle":"2024-07-08T16:35:17.878016Z","shell.execute_reply.started":"2024-07-08T16:35:17.871616Z","shell.execute_reply":"2024-07-08T16:35:17.877068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example usage:\n# report = polar_metrics_none(full_true_labels, full_predictions, aspect_names)\n# print(report) #1e5","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:24:48.280301Z","iopub.execute_input":"2024-06-24T18:24:48.280584Z","iopub.status.idle":"2024-06-24T18:24:48.343006Z","shell.execute_reply.started":"2024-06-24T18:24:48.280545Z","shell.execute_reply":"2024-06-24T18:24:48.342113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef predict(model, tokenizer, device, comments):\n    # Handle both single and multiple comments\n    if isinstance(comments, str):\n        comments = [comments]  # Ensure the input is iterable\n\n    model.eval()\n    inputs = tokenizer(comments, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)  # Outputs shape: (batch_size, num_aspects, num_polarities)\n#         print(outputs)\n        outputs= torch.softmax(outputs, dim=2)\n#         print(outputs)\n        predictions = torch.argmax(outputs, dim=2)  # Shape: (batch_size, num_aspects)\n#         print(predictions)\n        predictions = predictions.cpu().numpy()  # Convert to numpy array for easier handling\n#         print(predictions)\n    # Reverse mappings for aspects and sentiments\n    sentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\n    index_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\n    aspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4,\n                       'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\n    index_to_aspect = {v: k for k, v in aspect_to_index.items()}\n\n    predicted_aspects = []\n    # Iterate over each comment's predictions\n    for comment_predictions in predictions:\n        comment_aspects = []\n        for aspect_idx, sentiment_idx in enumerate(comment_predictions):\n            aspect = index_to_aspect[aspect_idx]\n            sentiment = index_to_sentiment[sentiment_idx]\n            comment_aspects.append((aspect, sentiment))\n        predicted_aspects.append(comment_aspects)\n\n    return predicted_aspects","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:02:35.673263Z","iopub.execute_input":"2024-07-16T03:02:35.674197Z","iopub.status.idle":"2024-07-16T03:02:35.686225Z","shell.execute_reply.started":"2024-07-16T03:02:35.674153Z","shell.execute_reply":"2024-07-16T03:02:35.685257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfTest['comment'][39]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:02:37.785664Z","iopub.execute_input":"2024-07-16T03:02:37.786483Z","iopub.status.idle":"2024-07-16T03:02:37.792312Z","shell.execute_reply.started":"2024-07-16T03:02:37.786453Z","shell.execute_reply":"2024-07-16T03:02:37.791436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment='máy đẹp chạy mượt_mà pin xài ổn sạc 3 tiếng đầy pin giá_cả hợp_lý vì mình có mua dung_lượng icloud nên 32gb xài củng tạm !'\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:02:40.018662Z","iopub.execute_input":"2024-07-16T03:02:40.019313Z","iopub.status.idle":"2024-07-16T03:02:40.023479Z","shell.execute_reply.started":"2024-07-16T03:02:40.019282Z","shell.execute_reply":"2024-07-16T03:02:40.022475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{BATTERY#Positive};{PERFORMANCE#Positive};{STORAGE#Neutral};{DESIGN#Positive};{PRICE#Positive};\n","metadata":{}},{"cell_type":"code","source":"comment= preprocess_text(comment)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:02:41.989509Z","iopub.execute_input":"2024-07-16T03:02:41.990336Z","iopub.status.idle":"2024-07-16T03:02:42.000812Z","shell.execute_reply.started":"2024-07-16T03:02:41.990307Z","shell.execute_reply":"2024-07-16T03:02:41.999862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_aspects2 = predict(model2, tokenizer, device, comment)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:02:47.818330Z","iopub.execute_input":"2024-07-16T03:02:47.818698Z","iopub.status.idle":"2024-07-16T03:02:47.854362Z","shell.execute_reply.started":"2024-07-16T03:02:47.818669Z","shell.execute_reply":"2024-07-16T03:02:47.853413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Predicted Aspects and Sentiments:\", predicted_aspects2) #1","metadata":{"execution":{"iopub.status.busy":"2024-07-16T03:05:02.384016Z","iopub.execute_input":"2024-07-16T03:05:02.385002Z","iopub.status.idle":"2024-07-16T03:05:02.389887Z","shell.execute_reply.started":"2024-07-16T03:05:02.384968Z","shell.execute_reply":"2024-07-16T03:05:02.388826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    sentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\n    index_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\n    aspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4,\n                       'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\n    index_to_aspect = {v: k for k, v in aspect_to_index.items()}","metadata":{"execution":{"iopub.status.busy":"2024-06-20T09:17:17.382523Z","iopub.execute_input":"2024-06-20T09:17:17.383386Z","iopub.status.idle":"2024-06-20T09:17:17.388829Z","shell.execute_reply.started":"2024-06-20T09:17:17.38335Z","shell.execute_reply":"2024-06-20T09:17:17.387903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NB","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report, f1_score,accuracy_score,recall_score,precision_score\n\n# Define mappings\nsentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\nindex_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\naspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4,\n                   'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\nindex_to_aspect = {v: k for k, v in aspect_to_index.items()}\n\n# Define the order of aspects\naspects = [index_to_aspect[i] for i in range(len(aspect_to_index))]\n\ndfTrainNB = dfTrain.copy()\ndfTestNB = dfTest.copy()\n# Convert tensor labels to integers\ndfTrainNB['sentiments'] = dfTrainNB['sentiments'].apply(lambda lst: [int(x.item()) for x in lst])\ndfTestNB['sentiments'] = dfTestNB['sentiments'].apply(lambda lst: [int(x.item()) for x in lst])\n\n# Extract text and sentiment labels\nX_train = dfTrainNB['comment']\nX_test = dfTestNB['comment']\n\n# Convert sentiment lists to individual columns\ny_train_sentiments = pd.DataFrame(dfTrainNB['sentiments'].tolist(), columns=[f'label_{aspect}' for aspect in aspects])\ny_test_sentiments = pd.DataFrame(dfTestNB['sentiments'].tolist(), columns=[f'label_{aspect}' for aspect in aspects])\n\n# Train the sentiment classification model for each aspect\nsentiment_pipelines= {}\n\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    sentiment_pipeline = make_pipeline(TfidfVectorizer(stop_words=None), MultinomialNB())\n    sentiment_pipeline.fit(X_train, y_train_sentiments[sentiment_column])\n    sentiment_pipelines[aspect] = sentiment_pipeline\n\n# Evaluate sentiment classification\noverall_sentiment_precision = []\noverall_sentiment_recall = []\noverall_sentiment_f1 = []\noverall_sentiment_accuracy = []\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    sentiment_pipeline = sentiment_pipelines[aspect]\n    y_pred = sentiment_pipeline.predict(X_test)\n    y_true = y_test_sentiments[sentiment_column]\n    \n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n    \n    print(f\"Sentiment Classification Report for {aspect.capitalize()}:\")\n    print(classification_report(y_test_sentiments[sentiment_column], y_pred, target_names=[index_to_sentiment[i] for i in range(4)], labels=[0, 1, 2, 3],zero_division=0))\n    overall_sentiment_precision.append(precision)\n    overall_sentiment_recall.append(recall)\n    overall_sentiment_f1.append(f1)\n    overall_sentiment_accuracy.append(accuracy)   \n\naverage_sentiment_precision = sum(overall_sentiment_precision) / len(overall_sentiment_precision)\naverage_sentiment_recall = sum(overall_sentiment_recall) / len(overall_sentiment_recall)\naverage_sentiment_f1 = sum(overall_sentiment_f1) / len(overall_sentiment_f1)\naverage_sentiment_accuracy = sum(overall_sentiment_accuracy) / len(overall_sentiment_accuracy)\n\nprint(\"\\nOverall Sentiment Classification Metrics:\")\nprint(f\"  Precision: {average_sentiment_precision:.4f}\")\nprint(f\"  Recall: {average_sentiment_recall:.4f}\")\nprint(f\"  F1-score: {average_sentiment_f1:.4f}\")\nprint(f\"  Accuracy: {average_sentiment_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T16:57:15.823985Z","iopub.execute_input":"2024-07-07T16:57:15.824604Z","iopub.status.idle":"2024-07-07T16:57:20.513942Z","shell.execute_reply.started":"2024-07-07T16:57:15.824573Z","shell.execute_reply":"2024-07-07T16:57:20.512951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:01:14.959552Z","iopub.execute_input":"2024-07-07T17:01:14.960335Z","iopub.status.idle":"2024-07-07T17:01:14.966602Z","shell.execute_reply.started":"2024-07-07T17:01:14.960296Z","shell.execute_reply":"2024-07-07T17:01:14.965558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sentiments = {}\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    sentiment_pipeline = sentiment_pipelines[aspect]\n    y_pred = sentiment_pipeline.predict(X_test)\n    pred_sentiments[sentiment_column] = y_pred\n\n# Convert sentiment predictions to aspect detection predictions\ndef sentiment_to_aspect_detection(y_true, y_pred):\n    true_aspect_detection = [1 if sentiment != 3 else 0 for sentiment in y_true]\n    pred_aspect_detection = [1 if sentiment != 3 else 0 for sentiment in y_pred]\n    return true_aspect_detection, pred_aspect_detection\n\n# Calculate and print aspect detection metrics for each aspect\nprint(\"Aspect Detection Metrics:\")\noverall_precision = []\noverall_recall = []\noverall_f1 = []\noverall_accuracy = []\n\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    true_aspect_detection, pred_aspect_detection = sentiment_to_aspect_detection(y_test_sentiments[sentiment_column], pred_sentiments[sentiment_column])\n    \n    accuracy = accuracy_score(true_aspect_detection, pred_aspect_detection)\n    precision = precision_score(true_aspect_detection, pred_aspect_detection, zero_division=0)\n    recall = recall_score(true_aspect_detection, pred_aspect_detection, zero_division=0)\n    f1 = f1_score(true_aspect_detection, pred_aspect_detection, zero_division=0)\n    \n    overall_precision.append(precision)\n    overall_recall.append(recall)\n    overall_f1.append(f1)\n    overall_accuracy.append(accuracy)\n    \n    print(f\"  {aspect} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Calculate overall metrics\naverage_precision = sum(overall_precision) / len(overall_precision)\naverage_recall = sum(overall_recall) / len(overall_recall)\naverage_f1 = sum(overall_f1) / len(overall_f1)\naverage_accuracy = sum(overall_accuracy) / len(overall_accuracy)\n\nprint(\"\\nOverall Aspect Detection Metrics:\")\nprint(f\"  Precision: {average_precision:.4f}\")\nprint(f\"  Recall: {average_recall:.4f}\")\nprint(f\"  F1-score: {average_f1:.4f}\")\nprint(f\"  Accuracy: {average_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:54:29.783086Z","iopub.execute_input":"2024-07-16T05:54:29.783774Z","iopub.status.idle":"2024-07-16T05:54:31.943902Z","shell.execute_reply.started":"2024-07-16T05:54:29.783733Z","shell.execute_reply":"2024-07-16T05:54:31.942823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, precision_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n# Define mappings\nsentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\nindex_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\naspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4,\n                   'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\nindex_to_aspect = {v: k for k, v in aspect_to_index.items()}\n\n# Define the order of aspects\naspects = [index_to_aspect[i] for i in range(len(aspect_to_index))]\n\ndfTrainNB = dfTrain.copy()\ndfTestNB = dfTest.copy()\n# Convert tensor labels to integers\ndfTrainNB['sentiments'] = dfTrainNB['sentiments'].apply(lambda lst: [int(x.item()) for x in lst])\ndfTestNB['sentiments'] = dfTestNB['sentiments'].apply(lambda lst: [int(x.item()) for x in lst])\n\n# Extract text and sentiment labels\nX_train = dfTrainNB['comment']\nX_test = dfTestNB['comment']\n\n# Convert sentiment lists to individual columns\ny_train_sentiments = pd.DataFrame(dfTrainNB['sentiments'].tolist(), columns=[f'label_{aspect}' for aspect in aspects])\ny_test_sentiments = pd.DataFrame(dfTestNB['sentiments'].tolist(), columns=[f'label_{aspect}' for aspect in aspects])\n\n# Train the sentiment classification model for each aspect\nsentiment_pipelines = {}\nparam_grid = {\n    'tfidfvectorizer__max_df': [0.75, 0.8, 0.85],\n    'tfidfvectorizer__min_df': [3, 5, 10],\n    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)],\n    'multinomialnb__alpha': [0.5, 1.0, 1.5]\n}\n\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    sentiment_pipeline = make_pipeline(\n        TfidfVectorizer(stop_words=None),\n        MultinomialNB()\n    )\n    \n    grid_search = GridSearchCV(sentiment_pipeline, param_grid, cv=5, scoring='f1_macro')\n    grid_search.fit(X_train, y_train_sentiments[sentiment_column])\n    \n    sentiment_pipelines[aspect] = grid_search.best_estimator_\n\n# Evaluate sentiment classification\noverall_sentiment_precision = []\noverall_sentiment_recall = []\noverall_sentiment_f1 = []\noverall_sentiment_accuracy = []\n\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    sentiment_pipeline = sentiment_pipelines[aspect]\n    y_pred = sentiment_pipeline.predict(X_test)\n    y_true = y_test_sentiments[sentiment_column]\n    \n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n    \n    print(f\"Sentiment Classification Report for {aspect.capitalize()}:\")\n    print(classification_report(y_test_sentiments[sentiment_column], y_pred, target_names=[index_to_sentiment[i] for i in range(4)], labels=[0, 1, 2, 3],zero_division=0))\n    overall_sentiment_precision.append(precision)\n    overall_sentiment_recall.append(recall)\n    overall_sentiment_f1.append(f1)\n    overall_sentiment_accuracy.append(accuracy)\n\naverage_sentiment_precision = sum(overall_sentiment_precision) / len(overall_sentiment_precision)\naverage_sentiment_recall = sum(overall_sentiment_recall) / len(overall_sentiment_recall)\naverage_sentiment_f1 = sum(overall_sentiment_f1) / len(overall_sentiment_f1)\naverage_sentiment_accuracy = sum(overall_sentiment_accuracy) / len(overall_sentiment_accuracy)\n\nprint(\"\\nOverall Sentiment Classification Metrics:\")\nprint(f\"  Precision: {average_sentiment_precision:.4f}\")\nprint(f\"  Recall: {average_sentiment_recall:.4f}\")\nprint(f\"  F1-score: {average_sentiment_f1:.4f}\")\nprint(f\"  Accuracy: {average_sentiment_accuracy:.4f}\")\n\npred_sentiments = {}\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    sentiment_pipeline = sentiment_pipelines[aspect]\n    y_pred = sentiment_pipeline.predict(X_test)\n    pred_sentiments[sentiment_column] = y_pred\n\n# Convert sentiment predictions to aspect detection predictions\ndef sentiment_to_aspect_detection(y_true, y_pred):\n    true_aspect_detection = [1 if sentiment != 3 else 0 for sentiment in y_true]\n    pred_aspect_detection = [1 if sentiment != 3 else 0 for sentiment in y_pred]\n    return true_aspect_detection, pred_aspect_detection\n\n# Calculate and print aspect detection metrics for each aspect\nprint(\"Aspect Detection Metrics:\")\noverall_precision = []\noverall_recall = []\noverall_f1 = []\noverall_accuracy = []\n\nfor aspect in aspects:\n    sentiment_column = f'label_{aspect}'\n    true_aspect_detection, pred_aspect_detection = sentiment_to_aspect_detection(y_test_sentiments[sentiment_column], pred_sentiments[sentiment_column])\n    \n    accuracy = accuracy_score(true_aspect_detection, pred_aspect_detection)\n    precision = precision_score(true_aspect_detection, pred_aspect_detection, zero_division=0)\n    recall = recall_score(true_aspect_detection, pred_aspect_detection, zero_division=0)\n    f1 = f1_score(true_aspect_detection, pred_aspect_detection, zero_division=0)\n    \n    overall_precision.append(precision)\n    overall_recall.append(recall)\n    overall_f1.append(f1)\n    overall_accuracy.append(accuracy)\n    \n    print(f\"  {aspect} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Calculate overall metrics\naverage_precision = sum(overall_precision) / len(overall_precision)\naverage_recall = sum(overall_recall) / len(overall_recall)\naverage_f1 = sum(overall_f1) / len(overall_f1)\naverage_accuracy = sum(overall_accuracy) / len(overall_accuracy)\n\nprint(\"\\nOverall Aspect Detection Metrics:\")\nprint(f\"  Precision: {average_precision:.4f}\")\nprint(f\"  Recall: {average_recall:.4f}\")\nprint(f\"  F1-score: {average_f1:.4f}\")\nprint(f\"  Accuracy: {average_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:51:32.402999Z","iopub.execute_input":"2024-07-16T04:51:32.403457Z","iopub.status.idle":"2024-07-16T05:16:49.573904Z","shell.execute_reply.started":"2024-07-16T04:51:32.403424Z","shell.execute_reply":"2024-07-16T05:16:49.572840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nSample Predictions:\")\nfor text in X_test:\n    print(f\"Text: {text}\")\n    for aspect in aspects:\n        sentiment_pipeline = sentiment_pipelines[aspect]\n        pred = sentiment_pipeline.predict([text])[0]\n        print(f\"  {aspect.capitalize()}: {index_to_sentiment[pred]}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:16:49.576518Z","iopub.execute_input":"2024-07-16T05:16:49.576982Z","iopub.status.idle":"2024-07-16T05:16:49.598558Z","shell.execute_reply.started":"2024-07-16T05:16:49.576943Z","shell.execute_reply":"2024-07-16T05:16:49.597467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_sentiments(comment):\n    predictions = {}\n    for aspect in aspects:\n        sentiment_pipeline = sentiment_pipelines[aspect]\n        sentiment_prediction = sentiment_pipeline.predict([comment])[0]\n        predictions[aspect] = index_to_sentiment[sentiment_prediction]\n    return predictions\n\n# Example usage of the function\ncomment = \"sản phẩm ok nhưng pin hơi yếu\"\npredicted_sentiments = predict_sentiments(comment)\n\nprint(\"Predicted Sentiments for the given comment:\")\nfor aspect, sentiment in predicted_sentiments.items():\n    print(f\"{aspect}: {sentiment}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:16:49.599631Z","iopub.execute_input":"2024-07-16T05:16:49.599990Z","iopub.status.idle":"2024-07-16T05:16:49.621192Z","shell.execute_reply.started":"2024-07-16T05:16:49.599962Z","shell.execute_reply":"2024-07-16T05:16:49.620295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.pipeline import make_pipeline\n# from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n\n# # Define mappings\n# sentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\n# index_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\n# aspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4,\n#                    'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9}\n# index_to_aspect = {v: k for k, v in aspect_to_index.items()}\n\n# aspects = [index_to_aspect[i] for i in range(len(aspect_to_index))]\n\n\n\n\n# # Convert tensor labels to integers\n# df['sentiments'] = df['sentiments'].apply(lambda lst: [int(x.item()) for x in lst])\n# dfTest['sentiments'] = dfTest['sentiments'].apply(lambda lst: [int(x.item()) for x in lst])\n\n\n# X_train = df['comment']\n# X_test = dfTest['comment']\n\n# y_train_sentiments = pd.DataFrame(df['sentiments'].tolist(), columns=[f'label_{aspect}' for aspect in aspects])\n# y_test_sentiments = pd.DataFrame(dfTest['sentiments'].tolist(), columns=[f'label_{aspect}' for aspect in aspects])\n\n# # Train \n# sentiment_pipelines = {}\n# for aspect in aspects:\n#     sentiment_column = f'label_{aspect}'\n#     sentiment_pipeline = make_pipeline(TfidfVectorizer(stop_words=None), MultinomialNB())\n#     sentiment_pipeline.fit(X_train, y_train_sentiments[sentiment_column])\n#     sentiment_pipelines[aspect] = sentiment_pipeline\n\n# # Predict \n# pred_sentiments = {}\n# for aspect in aspects:\n#     sentiment_column = f'label_{aspect}'\n#     sentiment_pipeline = sentiment_pipelines[aspect]\n#     y_pred = sentiment_pipeline.predict(X_test)\n#     pred_sentiments[sentiment_column] = y_pred\n\n# def sentiment_to_aspect_detection(y_true, y_pred):\n#     true_aspect_detection = [1 if sentiment != 3 else 0 for sentiment in y_true]\n#     pred_aspect_detection = [1 if sentiment != 3 else 0 for sentiment in y_pred]\n#     return true_aspect_detection, pred_aspect_detection\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:31:15.631541Z","iopub.execute_input":"2024-06-20T10:31:15.632264Z","iopub.status.idle":"2024-06-20T10:31:15.639008Z","shell.execute_reply.started":"2024-06-20T10:31:15.632229Z","shell.execute_reply":"2024-06-20T10:31:15.637947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# print(\"Aspect Detection Metrics:\")\n# overall_precision = []\n# overall_recall = []\n# overall_f1 = []\n# overall_accuracy = []\n\n# for aspect in aspects:\n#     sentiment_column = f'label_{aspect}'\n#     true_aspect_detection, pred_aspect_detection = sentiment_to_aspect_detection(y_test_sentiments[sentiment_column], pred_sentiments[sentiment_column])\n    \n#     accuracy = accuracy_score(true_aspect_detection, pred_aspect_detection)\n#     precision = precision_score(true_aspect_detection, pred_aspect_detection ,average='macro', zero_division=0)\n#     recall = recall_score(true_aspect_detection, pred_aspect_detection, average='macro', zero_division=0)\n#     f1 = f1_score(true_aspect_detection, pred_aspect_detection, average='macro', zero_division=0)\n    \n#     overall_precision.append(precision)\n#     overall_recall.append(recall)\n#     overall_f1.append(f1)\n#     overall_accuracy.append(accuracy)\n    \n#     print(f\"  {aspect} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n\n# average_precision = sum(overall_precision) / len(overall_precision)\n# average_recall = sum(overall_recall) / len(overall_recall)\n# average_f1 = sum(overall_f1) / len(overall_f1)\n# average_accuracy = sum(overall_accuracy) / len(overall_accuracy)\n\n# print(\"\\nOverall Aspect Detection Metrics:\")\n# print(f\"  Precision: {average_precision:.4f}\")\n# print(f\"  Recall: {average_recall:.4f}\")\n# print(f\"  F1-score: {average_f1:.4f}\")\n# print(f\"  Accuracy: {average_accuracy:.4f}\")\n\n# print(\"\\nSentiment Classification Metrics:\")\n# overall_sentiment_precision = []\n# overall_sentiment_recall = []\n# overall_sentiment_f1 = []\n# overall_sentiment_accuracy = []\n\n# for aspect in aspects:\n#     sentiment_column = f'label_{aspect}'\n#     y_true = y_test_sentiments[sentiment_column]\n#     y_pred = pred_sentiments[sentiment_column]\n    \n#     accuracy = accuracy_score(y_true, y_pred)\n#     precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n#     recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n#     f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n    \n#     overall_sentiment_precision.append(precision)\n#     overall_sentiment_recall.append(recall)\n#     overall_sentiment_f1.append(f1)\n#     overall_sentiment_accuracy.append(accuracy)\n    \n#     print(f\"  {aspect} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n#     print(classification_report(y_true, y_pred, target_names=[index_to_sentiment[i] for i in range(4)], labels=[0, 1, 2, 3], zero_division=0))\n\n# average_sentiment_precision = sum(overall_sentiment_precision) / len(overall_sentiment_precision)\n# average_sentiment_recall = sum(overall_sentiment_recall) / len(overall_sentiment_recall)\n# average_sentiment_f1 = sum(overall_sentiment_f1) / len(overall_sentiment_f1)\n# average_sentiment_accuracy = sum(overall_sentiment_accuracy) / len(overall_sentiment_accuracy)\n\n# print(\"\\nOverall Sentiment Classification Metrics:\")\n# print(f\"  Precision: {average_sentiment_precision:.4f}\")\n# print(f\"  Recall: {average_sentiment_recall:.4f}\")\n# print(f\"  F1-score: {average_sentiment_f1:.4f}\")\n# print(f\"  Accuracy: {average_sentiment_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:33:47.053082Z","iopub.execute_input":"2024-06-20T10:33:47.053451Z","iopub.status.idle":"2024-06-20T10:33:47.373296Z","shell.execute_reply.started":"2024-06-20T10:33:47.053421Z","shell.execute_reply":"2024-06-20T10:33:47.372212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Sample\n# print(\"\\nSample Predictions:\")\n# for text in X_test:\n#     print(f\"Text: {text}\")\n#     for aspect in aspects:\n#         sentiment_pipeline = sentiment_pipelines[aspect]\n#         pred = sentiment_pipeline.predict([text])[0]\n#         print(f\"  {aspect.capitalize()}: {index_to_sentiment[pred]}\")\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:31:25.190699Z","iopub.execute_input":"2024-06-20T10:31:25.191611Z","iopub.status.idle":"2024-06-20T10:31:25.211448Z","shell.execute_reply.started":"2024-06-20T10:31:25.191574Z","shell.execute_reply":"2024-06-20T10:31:25.21048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  multi: Precision: 0.7799, Recall: 0.7843, F1-score: 0.7645, Accuracy: 0.8127  \n  branch: Precision: 0.7812, Recall: 0.7684, F1-score: 0.7629, Accuracy: 0.8105  \n  concat: Precision: 0.7986, Recall: 0.7924, F1-score: 0.7834, Accuracy: 0.8228\n\n","metadata":{}},{"cell_type":"markdown","source":"## ABSA end","metadata":{}},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"# SA","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModel","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:55:18.389496Z","iopub.execute_input":"2024-07-16T05:55:18.390290Z","iopub.status.idle":"2024-07-16T05:55:18.394753Z","shell.execute_reply.started":"2024-07-16T05:55:18.390256Z","shell.execute_reply":"2024-07-16T05:55:18.393725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained('vinai/phobert-base-v2', output_hidden_states=True)\n        self.drop = nn.Dropout(p=0.2)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        hidden_states = outputs.last_hidden_state  # Tuple of hidden states\n        cls_output = hidden_states[:, 0]  # Taking the [CLS] token's representation\n        output = self.drop(cls_output)\n        logits = self.out(output) \n\n        return logits ","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:55:23.501812Z","iopub.execute_input":"2024-07-16T05:55:23.502737Z","iopub.status.idle":"2024-07-16T05:55:23.509744Z","shell.execute_reply.started":"2024-07-16T05:55:23.502701Z","shell.execute_reply":"2024-07-16T05:55:23.508648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelsa_path = r\"/kaggle/input/samodel/model4.pth\"\nsa_model=  SentimentClassifier(n_classes=3).to(device)\nsa_model.load_state_dict(torch.load(modelsa_path, map_location=device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictSA(model, tokenizer, device, comments):\n    model.eval()\n    inputs = tokenizer(comments, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        probabilities = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n        predictions = torch.argmax(probabilities, dim=1)  # Get predicted class\n        predictions = predictions.cpu().numpy()  # Convert to numpy array for easier handling\n\n    # Reverse mappings for sentiments\n    index_to_sentiment = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n    predicted_sentiments = [index_to_sentiment[idx] for idx in predictions]\n\n    return predicted_sentiments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = predict(model, tokenizer, device, comments)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the model\nmodel = NewapproachABSA1()\nmodel = model.to(device)\n\n# Load the trained model weights\nmodel_path = '/kaggle/input/test-demo/model7.pth'\nmodel.load_state_dict(torch.load(model_path, map_location=device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint = torch.load('model_checkpoint.pth', map_location=device)\n# model.load_state_dict(checkpoint['model_state_dict'])\n# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, device, comments):\n    model.eval()\n    inputs = tokenizer([comments], max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)  # Outputs shape: (1, num_aspects, num_polarities)\n        predictions = torch.argmax(outputs, dim=2)  # Shape: (1, num_aspects)\n        predictions = predictions.cpu().numpy()  # Convert to numpy array for easier handling\n\n    # Corrected sentiment mapping\n    sentiment_to_index = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'None': 3}\n    index_to_sentiment = {v: k for k, v in sentiment_to_index.items()}\n\n    aspect_to_index = {'GENERAL': 0, 'SER&ACC': 1, 'SCREEN': 2, 'CAMERA': 3, 'FEATURES': 4,\n                       'BATTERY': 5, 'PERFORMANCE': 6, 'STORAGE': 7, 'DESIGN': 8, 'PRICE': 9, 'OTHERS': 10}\n    index_to_aspect = {v: k for k, v in aspect_to_index.items()}\n\n    predicted_aspects = []\n    # Assuming single comment prediction, extract predictions for each aspect\n    for aspect_idx, sentiment_idx in enumerate(predictions[0]):\n        aspect = index_to_aspect[aspect_idx]\n        sentiment = index_to_sentiment[sentiment_idx]\n        predicted_aspects.append((aspect, sentiment))\n\n    return predicted_aspects","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments ='sản_phẩm tốt chất_lượng có_thể thay cho tai nghe havit i62 , shop gói hàng cẩn_thận chu_đáo trả_lời tin nhắn nhiệt_tình'\ncomments1= 'chất_lượng sản_phẩm : okla hàng mall thì khỏi_phải_nói r he , thay vào như mới rcm cực mạnh mua về thay cho tai nghe'\ncomments2='tính_năng nổi_bật : ôm sát tai hạn_chế đau tai chất_lượng sản_phẩm : ổn cũng dễ gắn ra vô lắm êm , mềm , đẹp nha cho 10đ nhưng_mà phải gỡ cẩn_thận chứ không dễ bị gãy mắc cài tai nghe lắm mn'\ncomments3='chất_lượng sản_phẩm : ok tính_năng nổi_bật : ok shop rep inb nhanh , nhiệt_tình , đồ giao cũng nhanh , hàng chuẩn , cách_âm tốt , giá ok và chất_lượng cũng khá ổn , sẽ còn ủng_hộ shop nhiều , màu xinh quá'\ncomments= preprocess_text(comments)\ncomments1= preprocess_text(comments1)\ncomments2= preprocess_text(comments2)\ncomments3= preprocess_text(comments3)\n# test ='Tivi to đẹp, màn hình rõ fullhd, nhưng loa nghe hơi rè, ship nhanh, đáng đồng tiền, '\ntest= 'tai nghe vừa tai không bị đau hay chật, bass rõ ràng nghe rất thích, giá ok '\ntest=preprocess_text(test)\n\npredicted_aspects = predict(model, tokenizer, device, comments)\npredicted_aspects1 = predict(model, tokenizer, device, comments1)\npredicted_aspects2 = predict(model, tokenizer, device, comments2)\npredicted_aspects3 = predict(model, tokenizer, device, comments3)\ntest = predict(model, tokenizer, device, test)\n\nprint(\"\\nPredicted Aspects and Sentiments:\\n\", predicted_aspects)\nprint(\"\\nPredicted Aspects and Sentiments:\\n\", predicted_aspects1)\nprint(\"\\nPredicted Aspects and Sentiments:\\n\", predicted_aspects2)\nprint(\"\\nPredicted Aspects and Sentiments:\\n\", predicted_aspects3)\nprint(\"\\nPredicted Aspects and Sentiments:\\n\", test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments ='sanr phẩm đẹp, chụp ảnh sịn nhưng pin hơi yếu dùng được có 2 tiếng là hết rồi'\ncomments= preprocess_text(comments)\n\n\npredicted_aspects = predict(model, tokenizer, device, comments)\nprint(\"Predicted Aspects and Sentiments:\", predicted_aspects)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'][5555]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['comment'][5555]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'{FEATURES#Negative};{PRICE#Neutral};{GENERAL#Positive};'\n","metadata":{}},{"cell_type":"markdown","source":"# SA test","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:55:59.141380Z","iopub.execute_input":"2024-07-16T05:55:59.141832Z","iopub.status.idle":"2024-07-16T05:55:59.157493Z","shell.execute_reply.started":"2024-07-16T05:55:59.141798Z","shell.execute_reply":"2024-07-16T05:55:59.156667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=75):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer.encode_plus(\n          text,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          padding='max_length',\n          return_attention_mask=True,\n          return_tensors='pt',\n          truncation=True\n        )\n\n        return {\n          'text': text,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'sentiments': torch.tensor(label, dtype=torch.long)  # Changed 'labels' to 'sentiments'\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:56:08.950196Z","iopub.execute_input":"2024-07-16T05:56:08.950573Z","iopub.status.idle":"2024-07-16T05:56:08.959190Z","shell.execute_reply.started":"2024-07-16T05:56:08.950544Z","shell.execute_reply":"2024-07-16T05:56:08.958208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained('vinai/phobert-base-v2', output_hidden_states=True)\n        self.drop = nn.Dropout(p=0.2)\n        # Multipling the hidden size by 4 because we concatenate the last four layers\n        self.out = nn.Linear(self.bert.config.hidden_size * 4, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        # Concatenate the last four hidden states\n        hidden_states = outputs.hidden_states\n        concat_hidden = torch.cat(tuple(hidden_states[-i] for i in range(1, 5)), dim=-1)\n        # We use the output of the concatenated last layer's [CLS] tokens\n        cls_output = concat_hidden[:, 0, :]  # Taking the [CLS] token's representation\n        output = self.drop(cls_output)\n        # logits = self.classifier(cls_output) # This should be corrected as below\n        logits = self.out(output)  # Changed to `self.out`\n\n        return logits  ","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:01:09.993677Z","iopub.execute_input":"2024-07-16T08:01:09.994078Z","iopub.status.idle":"2024-07-16T08:01:10.004982Z","shell.execute_reply.started":"2024-07-16T08:01:09.994049Z","shell.execute_reply":"2024-07-16T08:01:10.003799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from transformers import AutoModel\n\n# class SentimentClassifier1(nn.Module):\n#     def __init__(self, n_classes):\n#         super(SentimentClassifier1, self).__init__()\n#         self.bert = AutoModel.from_pretrained('vinai/phobert-base', output_hidden_states=True)\n#         self.drop = nn.Dropout(p=0.5)\n#         # Multipling the hidden size by 4 because we concatenate the last four layers\n#         self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n#     def forward(self, input_ids, attention_mask):\n#         outputs = self.bert(\n#           input_ids=input_ids,\n#           attention_mask=attention_mask\n#         )\n#         # Concatenate the last four hidden states\n#         hidden_states = outputs.last_hidden_state  # Tuple of hidden states\n# #         concat_hidden = torch.cat(tuple(hidden_states[-i] for i in range(1, 5)), dim=-1)\n#         # We use the output of the concatenated last layer's [CLS] tokens\n#         cls_output = hidden_states[:, 0]  # Taking the [CLS] token's representation\n#         output = self.drop(cls_output)\n#         # logits = self.classifier(cls_output) # This should be corrected as below\n#         logits = self.out(output)  # Changed to `self.out`\n\n#         return logits ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_sadataset = CSADataset(dfSA_train['comment'].tolist(), dfSA_train['label'].tolist())\n# train_saloader = DataLoader(train_sadataset, batch_size=32, shuffle=True)\n# val_sadataset = CSADataset(dfSA_val['comment'].tolist(), dfSA_val['label'].tolist())\n# val_saloader = DataLoader(val_sadataset, batch_size=32, shuffle=False) \n# test_sadataset = CSADataset(dfSA_test['comment'].tolist(), dfSA_test['label'].tolist())\n# test_saloader = DataLoader(test_sadataset, batch_size=32, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfSA_test['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_saloader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:58:47.538762Z","iopub.execute_input":"2024-07-16T05:58:47.539107Z","iopub.status.idle":"2024-07-16T05:58:47.569020Z","shell.execute_reply.started":"2024-07-16T05:58:47.539082Z","shell.execute_reply":"2024-07-16T05:58:47.568007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=20, patience=5):\n    criterion = torch.nn.CrossEntropyLoss()\n    best_val_loss = float('inf')\n    epochs_without_improvement = 0\n    train_losses=[]\n    val_losses=[]\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)   #attention labels and sentiments different\n            #print(labels,'\\n')\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            #print(outputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            total_loss += loss.item()\n        scheduler.step()\n        avg_train_loss = total_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}')\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n\n        print(f'Validation Loss: {avg_val_loss:.4f}')\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            epochs_without_improvement = 0\n        \n        else:\n            epochs_without_improvement += 1\n            print(f'no improvement after {epochs_without_improvement} epochs')\n            if epochs_without_improvement >= patience:\n                print(\"Early stopping...\")\n                break\n    return  train_losses, val_losses","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:01:15.412985Z","iopub.execute_input":"2024-07-16T06:01:15.414177Z","iopub.status.idle":"2024-07-16T06:01:15.432541Z","shell.execute_reply.started":"2024-07-16T06:01:15.414135Z","shell.execute_reply":"2024-07-16T06:01:15.431297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:01:23.042758Z","iopub.execute_input":"2024-07-16T06:01:23.043395Z","iopub.status.idle":"2024-07-16T06:01:23.050023Z","shell.execute_reply.started":"2024-07-16T06:01:23.043362Z","shell.execute_reply":"2024-07-16T06:01:23.048884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifier(n_classes=3).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:14:44.765339Z","iopub.execute_input":"2024-07-16T07:14:44.765760Z","iopub.status.idle":"2024-07-16T07:14:45.280766Z","shell.execute_reply.started":"2024-07-16T07:14:44.765726Z","shell.execute_reply":"2024-07-16T07:14:45.279661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\ntotal_steps = len(train_loader) * 20\n# Assuming 20 epochs","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:14:54.271647Z","iopub.execute_input":"2024-07-16T07:14:54.272364Z","iopub.status.idle":"2024-07-16T07:14:54.277177Z","shell.execute_reply.started":"2024-07-16T07:14:54.272328Z","shell.execute_reply":"2024-07-16T07:14:54.276157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=1e-6)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:14:57.601814Z","iopub.execute_input":"2024-07-16T07:14:57.602217Z","iopub.status.idle":"2024-07-16T07:14:57.610795Z","shell.execute_reply.started":"2024-07-16T07:14:57.602186Z","shell.execute_reply":"2024-07-16T07:14:57.609848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses = train(model, train_saloader,val_saloader, optimizer, scheduler, device, num_epochs=0, patience=5)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:15:05.038389Z","iopub.execute_input":"2024-07-16T07:15:05.039110Z","iopub.status.idle":"2024-07-16T07:55:26.788646Z","shell.execute_reply.started":"2024-07-16T07:15:05.039077Z","shell.execute_reply":"2024-07-16T07:55:26.787581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_losses(train_losses, val_losses):\n    import matplotlib.pyplot as plt\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Train Loss', color='red')\n    plt.plot(val_losses, label='Validation Loss', color='blue')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:55:26.790560Z","iopub.execute_input":"2024-07-16T07:55:26.790893Z","iopub.status.idle":"2024-07-16T07:55:26.797067Z","shell.execute_reply.started":"2024-07-16T07:55:26.790865Z","shell.execute_reply":"2024-07-16T07:55:26.796194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses, val_losses) #0.2","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:55:26.798234Z","iopub.execute_input":"2024-07-16T07:55:26.798523Z","iopub.status.idle":"2024-07-16T07:55:27.113529Z","shell.execute_reply.started":"2024-07-16T07:55:26.798492Z","shell.execute_reply":"2024-07-16T07:55:27.112356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_losses(train_losses, val_losses)  #0.3","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:25:41.239037Z","iopub.execute_input":"2024-07-16T06:25:41.239376Z","iopub.status.idle":"2024-07-16T06:25:41.244011Z","shell.execute_reply.started":"2024-07-16T06:25:41.239347Z","shell.execute_reply":"2024-07-16T06:25:41.242899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            # Ensure this is the correct key for your labels. Use 'labels' or 'sentiments' based on your dataset setup\n#             labels = batch.get('sentiments', batch.get('labels')).to(device)\n            labels = batch.get('labels')  # Assuming 'labels' is the consistent key used\n            if labels is None:\n                raise ValueError(\"Label key 'labels' not found in the batch\")\n            labels = labels.to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            probabilities = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n\n            predictions = torch.argmax(probabilities, dim=1)  # Assuming the output is [batch_size, num_classes]\n\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate overall metrics\n    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro') #weighted, macro, micro\n    accuracy = accuracy_score(all_labels, all_predictions)\n\n    # Return predictions and true labels, along with calculated metrics\n    return all_predictions, all_labels, {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1_score\n    }","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:12:17.474935Z","iopub.execute_input":"2024-07-16T07:12:17.475347Z","iopub.status.idle":"2024-07-16T07:12:17.486163Z","shell.execute_reply.started":"2024-07-16T07:12:17.475317Z","shell.execute_reply":"2024-07-16T07:12:17.485328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in test_saloader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:12:18.637541Z","iopub.execute_input":"2024-07-16T07:12:18.637939Z","iopub.status.idle":"2024-07-16T07:12:18.656071Z","shell.execute_reply.started":"2024-07-16T07:12:18.637908Z","shell.execute_reply":"2024-07-16T07:12:18.654841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_to_index = {'Negative': 0, 'Positive': 1, 'Neutral': 2}  \n\npredictions, true_labels, metrics = evaluate(model, test_saloader, device)\n# print(\"Predictions:\", predictions)\n# print(\"True Labels:\", true_labels)\nprint(\"Metrics:\", metrics)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:01:24.058141Z","iopub.execute_input":"2024-07-16T08:01:24.058559Z","iopub.status.idle":"2024-07-16T08:01:33.799953Z","shell.execute_reply.started":"2024-07-16T08:01:24.058528Z","shell.execute_reply":"2024-07-16T08:01:33.798993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4 last\ndisplay_metrics(true_labels, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:01:51.233280Z","iopub.execute_input":"2024-07-16T08:01:51.233675Z","iopub.status.idle":"2024-07-16T08:01:51.257765Z","shell.execute_reply.started":"2024-07-16T08:01:51.233646Z","shell.execute_reply":"2024-07-16T08:01:51.256656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_metrics(full_true_labels, full_predictions):\n    print(\"Classification Report:\")\n    print(classification_report(full_true_labels, full_predictions, target_names=['Negative', 'Positive', 'Neutral'], zero_division=0))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:00.429421Z","iopub.execute_input":"2024-07-16T08:02:00.430139Z","iopub.status.idle":"2024-07-16T08:02:00.435354Z","shell.execute_reply.started":"2024-07-16T08:02:00.430103Z","shell.execute_reply":"2024-07-16T08:02:00.434357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, device, comments):\n    model.eval()\n    inputs = tokenizer(comments, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        probabilities = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n        predictions = torch.argmax(probabilities, dim=1)  # Get predicted class\n        predictions = predictions.cpu().numpy()  # Convert to numpy array for easier handling\n\n    # Reverse mappings for sentiments\n    index_to_sentiment = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n    predicted_sentiments = [index_to_sentiment[idx] for idx in predictions]\n\n    return predicted_sentiments\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:07.531136Z","iopub.execute_input":"2024-07-16T08:02:07.531857Z","iopub.status.idle":"2024-07-16T08:02:07.539355Z","shell.execute_reply.started":"2024-07-16T08:02:07.531824Z","shell.execute_reply":"2024-07-16T08:02:07.538391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['comment'][9981]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:17.362584Z","iopub.execute_input":"2024-07-16T08:02:17.362990Z","iopub.status.idle":"2024-07-16T08:02:17.367714Z","shell.execute_reply.started":"2024-07-16T08:02:17.362961Z","shell.execute_reply":"2024-07-16T08:02:17.366525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['label'][9981]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:19.425995Z","iopub.execute_input":"2024-07-16T08:02:19.426792Z","iopub.status.idle":"2024-07-16T08:02:19.430886Z","shell.execute_reply.started":"2024-07-16T08:02:19.426759Z","shell.execute_reply":"2024-07-16T08:02:19.429833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments='máy đang xài bị đơ màng hình bấm không được khoảng 5p ms trở_lại bình_thường mua gần được 1 tháng'","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:21.735561Z","iopub.execute_input":"2024-07-16T08:02:21.736278Z","iopub.status.idle":"2024-07-16T08:02:21.740504Z","shell.execute_reply.started":"2024-07-16T08:02:21.736237Z","shell.execute_reply":"2024-07-16T08:02:21.739468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model, tokenizer, device, comments)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:27.214695Z","iopub.execute_input":"2024-07-16T08:02:27.215652Z","iopub.status.idle":"2024-07-16T08:02:27.239801Z","shell.execute_reply.started":"2024-07-16T08:02:27.215616Z","shell.execute_reply":"2024-07-16T08:02:27.238794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model4, tokenizer, device, comments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model4, tokenizer, device, comments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model4, tokenizer, device, comments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install torchviz\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:55:42.280873Z","iopub.execute_input":"2024-06-24T08:55:42.281602Z","iopub.status.idle":"2024-06-24T08:55:57.632561Z","shell.execute_reply.started":"2024-06-24T08:55:42.281567Z","shell.execute_reply":"2024-06-24T08:55:57.631498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from transformers import AutoModel\n# from torchviz import make_dot\n\n# class MultitaskABSAConcat(nn.Module):\n#     def __init__(self, num_aspects=10, num_polarities=4):\n#         super().__init__()\n#         self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\", output_hidden_states=True)\n#         self.dropout = nn.Dropout(0.5)\n#         self.aspect_classifiers = nn.ModuleList([\n#             nn.Linear(self.phobert.config.hidden_size * 4, num_polarities) for _ in range(num_aspects)\n#         ])\n#         self.num_aspects = num_aspects\n#         self.num_polarities = num_polarities\n\n#     def forward(self, input_ids, attention_mask):\n#         outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n#         concat_hidden = torch.cat(outputs.hidden_states[-4:], dim=-1)  # Shape: (batch_size, seq_len, hidden_size*4)\n#         cls_output = concat_hidden[:, 0, :]  # Shape: (batch_size, hidden_size*4)\n#         cls_output = self.dropout(cls_output)\n        \n#         # Calculate logits for each aspect\n#         aspect_logits = [classifier(cls_output) for classifier in self.aspect_classifiers]\n        \n#         # Concatenate the aspect logits along the last dimension\n#         logits = torch.cat(aspect_logits, dim=1)\n#         logits = logits.view(-1, self.num_aspects, self.num_polarities)\n\n#         return logits\n\n# # Tạo một ví dụ đầu vào\n# input_ids = torch.randint(0, 100, (1, 128))  # batch_size=1, seq_len=128\n# attention_mask = torch.ones_like(input_ids)\n\n# # Khởi tạo mô hình và tạo đồ thị\n# model = MultitaskABSAConcat(num_aspects=10, num_polarities=4)\n# logits = model(input_ids, attention_mask)\n\n# # Trực quan hóa mô hình\n# dot = make_dot(logits, params=dict(model.named_parameters()))\n# dot.format = 'png'\n# dot.render('model_architecture')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:55:58.396744Z","iopub.execute_input":"2024-06-24T08:55:58.397135Z","iopub.status.idle":"2024-06-24T08:56:16.343535Z","shell.execute_reply.started":"2024-06-24T08:55:58.397103Z","shell.execute_reply":"2024-06-24T08:56:16.342637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NB","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Calculate TF-IDF\ntf_idf = TfidfVectorizer(smooth_idf=False, min_df=3)\nX_train_tfidf = tf_idf.fit_transform(dfSA_train['comment'])\ny_train= dfSA_train['label']\nX_val_tfidf= tf_idf.transform(dfSA_test['comment'])\ny_val=dfSA_test['label']","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:36.277090Z","iopub.execute_input":"2024-07-16T08:02:36.277915Z","iopub.status.idle":"2024-07-16T08:02:37.089453Z","shell.execute_reply.started":"2024-07-16T08:02:36.277881Z","shell.execute_reply":"2024-07-16T08:02:37.088662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, roc_auc_score\n\ndef get_auc_CV(model, X, y, cv=5, scoring='roc_auc_ovo'):\n    roc_auc_scorer = make_scorer(roc_auc_score, multi_class='ovo', needs_proba=True)\n    scores = cross_val_score(model, X, y, cv=cv, scoring=roc_auc_scorer)\n    return scores.mean()\n\nalpha_range = np.arange(0.01, 10, 0.1)\n\nres = pd.Series(\n    [get_auc_CV(MultinomialNB(alpha=i), X_train_tfidf, y_train) for i in alpha_range],\n    index=alpha_range\n)\n\n# Find the best alpha\nbest_alpha = np.round(res.idxmax(), 2)\nprint('Best alpha: ', best_alpha)\n\n# Plot AUC vs. Alpha\nplt.figure(figsize=(10, 5))\nplt.plot(res)\nplt.title('AUC vs. Alpha')\nplt.xlabel('Alpha')\nplt.ylabel('AUC')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:02:42.176339Z","iopub.execute_input":"2024-07-16T08:02:42.177180Z","iopub.status.idle":"2024-07-16T08:02:54.611003Z","shell.execute_reply.started":"2024-07-16T08:02:42.177146Z","shell.execute_reply":"2024-07-16T08:02:54.610097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb = MultinomialNB(alpha = 0.41,force_alpha = True, fit_prior = False)\nnb.fit(X_train_tfidf,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:05:46.035794Z","iopub.execute_input":"2024-07-16T08:05:46.036718Z","iopub.status.idle":"2024-07-16T08:05:46.057393Z","shell.execute_reply.started":"2024-07-16T08:05:46.036684Z","shell.execute_reply":"2024-07-16T08:05:46.056355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score\n\ndef eval(model, X_train, y_train, X_test, y_test):\n    # Predict on training and test data\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    # Evaluate the performance on the training set\n    print(\"Training Set Evaluation:\")\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_train, y_pred_train))\n    print(\"Classification Report:\")\n    print(classification_report(y_train, y_pred_train))\n\n    # Evaluate the performance on the test set\n    print(\"Test Set Evaluation:\")\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred_test))\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred_test))\n    return y_test, y_pred_test","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:05:51.155434Z","iopub.execute_input":"2024-07-16T08:05:51.156370Z","iopub.status.idle":"2024-07-16T08:05:51.165896Z","shell.execute_reply.started":"2024-07-16T08:05:51.156331Z","shell.execute_reply":"2024-07-16T08:05:51.164560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"NB MODEL\\n\")\ny_test, y_pred_test=eval(nb, X_train_tfidf, y_train, X_val_tfidf, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:05:53.781227Z","iopub.execute_input":"2024-07-16T08:05:53.781631Z","iopub.status.idle":"2024-07-16T08:05:53.849252Z","shell.execute_reply.started":"2024-07-16T08:05:53.781585Z","shell.execute_reply":"2024-07-16T08:05:53.848256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_metrics(full_true_labels, full_predictions):\n    print(\"Classification Report:\")\n    print(classification_report(full_true_labels, full_predictions, target_names=['Negative', 'Positive', 'Neutral'], zero_division=0))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:06:01.339261Z","iopub.execute_input":"2024-07-16T08:06:01.339689Z","iopub.status.idle":"2024-07-16T08:06:01.345184Z","shell.execute_reply.started":"2024-07-16T08:06:01.339623Z","shell.execute_reply":"2024-07-16T08:06:01.344088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_metrics(y_test, y_pred_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:06:03.377584Z","iopub.execute_input":"2024-07-16T08:06:03.378557Z","iopub.status.idle":"2024-07-16T08:06:03.398778Z","shell.execute_reply.started":"2024-07-16T08:06:03.378522Z","shell.execute_reply":"2024-07-16T08:06:03.397676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, device, comments):\n    model.eval()\n    inputs = tokenizer(comments, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        probabilities = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n        predictions = torch.argmax(probabilities, dim=1)  # Get predicted class\n        predictions = predictions.cpu().numpy()  # Convert to numpy array for easier handling\n\n    # Reverse mappings for sentiments\n    index_to_sentiment = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n    predicted_sentiments = [index_to_sentiment[idx] for idx in predictions]\n\n    return predicted_sentiments","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:06:10.700894Z","iopub.execute_input":"2024-07-16T08:06:10.701276Z","iopub.status.idle":"2024-07-16T08:06:10.711921Z","shell.execute_reply.started":"2024-07-16T08:06:10.701241Z","shell.execute_reply":"2024-07-16T08:06:10.710678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model, tokenizer, device, comments)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T08:06:11.867105Z","iopub.execute_input":"2024-07-16T08:06:11.867511Z","iopub.status.idle":"2024-07-16T08:06:11.892435Z","shell.execute_reply.started":"2024-07-16T08:06:11.867481Z","shell.execute_reply":"2024-07-16T08:06:11.891522Z"},"trusted":true},"execution_count":null,"outputs":[]}]}